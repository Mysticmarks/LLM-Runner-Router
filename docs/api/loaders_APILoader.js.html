<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: loaders/APILoader.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: loaders/APILoader.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * ðŸŒ API Model Loader
 * Universal loader for API-based LLM providers (OpenAI, Anthropic, OpenRouter, Groq)
 * Echo AI Systems - Bringing cloud AI to your local orchestration
 */

import BaseLoader from './BaseLoader.js';
import { Logger } from '../utils/Logger.js';
import { StreamProcessor } from '../runtime/StreamProcessor.js';

const logger = new Logger('APILoader');

/**
 * API Provider configurations
 * Comprehensive configuration for all 24 supported LLM providers
 */
const PROVIDER_CONFIGS = {
  // Existing providers
  openai: {
    baseURL: 'https://api.openai.com/v1',
    models: ['gpt-4-turbo-preview', 'gpt-4', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k'],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 10, output: 30 },
    features: ['function_calling', 'vision', 'json_mode']
  },
  anthropic: {
    baseURL: 'https://api.anthropic.com/v1',
    models: ['claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307', 'claude-2.1', 'claude-instant-1.2'],
    headers: (apiKey) => ({
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 15, output: 75 },
    features: ['long_context', 'safety_focus']
  },
  openrouter: {
    baseURL: 'https://openrouter.ai/api/v1',
    models: ['auto'], // OpenRouter supports many models dynamically
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
      'HTTP-Referer': 'https://github.com/MCERQUA/LLM-Runner-Router',
      'X-Title': 'LLM-Runner-Router'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 0, output: 0 }, // OpenRouter provides per-model pricing
    features: ['multi_provider', 'model_routing']
  },
  groq: {
    baseURL: 'https://api.groq.com/openai/v1',
    models: ['mixtral-8x7b-32768', 'llama2-70b-4096', 'gemma-7b-it'],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 0.27, output: 0.27 }, // Groq's fast inference pricing
    features: ['ultra_fast', 'lpu_technology']
  },

  // Phase 1: Enterprise Cloud Giants
  bedrock: {
    baseURL: null, // Uses AWS SDK
    models: [
      'anthropic.claude-3-opus-20240229-v1:0',
      'anthropic.claude-3-sonnet-20240229-v1:0', 
      'anthropic.claude-3-haiku-20240307-v1:0',
      'meta.llama2-70b-chat-v1',
      'mistral.mixtral-8x7b-instruct-v0:1',
      'amazon.titan-text-express-v1',
      'cohere.command-text-v14'
    ],
    headers: () => ({
      'Content-Type': 'application/json',
      'User-Agent': 'LLM-Runner-Router/2.0.0'
    }),
    streaming: true,
    authType: 'aws_sdk',
    costPerMillion: { input: 15, output: 75 }, // Varies by model
    features: ['enterprise', 'aws_native', 'multi_model']
  },
  'azure-openai': {
    baseURL: null, // Set dynamically from endpoint
    models: ['gpt-4-turbo', 'gpt-4', 'gpt-35-turbo', 'gpt-35-turbo-16k', 'dall-e-3', 'text-embedding-ada-002'],
    headers: (apiKey) => ({
      'api-key': apiKey,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key', // or azure_sdk
    costPerMillion: { input: 10, output: 30 }, // Same as OpenAI
    features: ['enterprise', 'compliance', 'azure_ad', 'hipaa', 'soc2']
  },
  'vertex-ai': {
    baseURL: null, // Set dynamically from project/location
    models: [
      'gemini-1.5-pro',
      'gemini-1.0-pro',
      'gemini-1.0-pro-vision',
      'text-bison',
      'chat-bison',
      'code-bison',
      'textembedding-gecko'
    ],
    headers: () => ({
      'Content-Type': 'application/json',
      'User-Agent': 'LLM-Runner-Router/2.0.0'
    }),
    streaming: true,
    authType: 'gcp_sdk',
    costPerMillion: { input: 3.5, output: 10.5 }, // Varies by model
    features: ['multimodal', 'enterprise', 'gcp_native', 'mlops']
  },
  mistral: {
    baseURL: 'https://api.mistral.ai/v1',
    models: [
      'mistral-large-latest',
      'mistral-medium-latest', 
      'mistral-small-latest',
      'mistral-tiny',
      'open-mistral-7b',
      'open-mixtral-8x7b',
      'open-mixtral-8x22b',
      'mistral-embed'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 8, output: 24 }, // Varies by model
    features: ['european', 'gdpr_compliant', 'multilingual', 'function_calling']
  },

  // Phase 2: High-Performance Inference
  together: {
    baseURL: 'https://api.together.xyz/v1',
    models: [
      'meta-llama/Llama-2-70b-chat-hf',
      'meta-llama/Llama-2-13b-chat-hf',
      'meta-llama/CodeLlama-34b-Instruct-hf',
      'mistralai/Mixtral-8x7B-Instruct-v0.1',
      'mistralai/Mistral-7B-Instruct-v0.1'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 0.9, output: 0.9 }, // Varies by model
    features: ['open_source', 'batch_processing', 'fine_tuning']
  },
  fireworks: {
    baseURL: 'https://api.fireworks.ai/inference/v1',
    models: [
      'accounts/fireworks/models/llama-v3p1-70b-instruct',
      'accounts/fireworks/models/llama-v3p1-8b-instruct',
      'accounts/fireworks/models/mixtral-8x7b-instruct',
      'accounts/fireworks/models/firefunction-v2'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 0.9, output: 0.9 }, // Varies by model
    features: ['fire_attention', 'function_calling', 'structured_output', 'hipaa', 'soc2']
  },
  deepinfra: {
    baseURL: 'https://api.deepinfra.com/v1/openai',
    models: [
      'meta-llama/Llama-2-70b-chat-hf',
      'mistralai/Mixtral-8x7B-Instruct-v0.1',
      'codellama/CodeLlama-34b-Instruct-hf'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 0.27, output: 0.27 }, // 50% cost savings
    features: ['cost_effective', 'gpu_optimization', 'serverless']
  },
  replicate: {
    baseURL: 'https://api.replicate.com/v1',
    models: [
      'meta/llama-2-70b-chat',
      'mistralai/mixtral-8x7b-instruct-v0.1',
      'stability-ai/stable-diffusion'
    ],
    headers: (apiKey) => ({
      'Authorization': `Token ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: false, // Replicate uses prediction polling
    authType: 'api_key',
    costPerMillion: { input: 0.65, output: 2.75 },
    features: ['community_models', 'version_control', 'async_predictions']
  },

  // Phase 3: Specialized &amp; Multi-Modal
  cohere: {
    baseURL: 'https://api.cohere.ai/v1',
    models: [
      'command-r-plus',
      'command-r',
      'command',
      'command-light',
      'embed-english-v3.0',
      'embed-multilingual-v3.0',
      'rerank-english-v3.0',
      'rerank-multilingual-v3.0'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
      'Cohere-Version': '2022-12-06'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 3.0, output: 15.0 }, // Command R+ pricing
    features: ['embeddings', 'reranking', 'tool_use', 'retrieval', 'multilingual', 'enterprise']
  },
  perplexity: {
    baseURL: 'https://api.perplexity.ai',
    models: [
      'llama-3.1-sonar-large-128k-online',
      'llama-3.1-sonar-small-128k-online',
      'llama-3.1-sonar-huge-128k-online',
      'llama-3.1-sonar-large-128k-chat',
      'llama-3.1-sonar-small-128k-chat',
      'llama-3.1-8b-instruct',
      'llama-3.1-70b-instruct'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 1.0, output: 1.0 }, // Sonar Large pricing
    features: ['web_search', 'real_time_info', 'citations', 'factual_accuracy']
  },
  deepseek: {
    baseURL: 'https://api.deepseek.com/v1',
    models: [
      'deepseek-chat',
      'deepseek-coder',
      'deepseek-reasoner',
      'deepseek-math'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true,
    authType: 'api_key',
    costPerMillion: { input: 0.14, output: 0.28 }, // Ultra cost-effective
    features: ['cost_optimization', 'reasoning', 'coding', 'mathematics', 'chain_of_thought']
  },
  novita: {
    baseURL: 'https://api.novita.ai/v3',
    models: [
      'llama-3.1-8b-instruct',
      'llama-3.1-70b-instruct',
      'llama-3.1-405b-instruct',
      'flux-dev',
      'flux-schnell',
      'sdxl-turbo',
      'kling-video',
      'runway-gen3',
      'elevenlabs-tts',
      'openai-tts'
    ],
    headers: (apiKey) => ({
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }),
    streaming: true, // For text models
    authType: 'api_key',
    costPerMillion: { input: 0.59, output: 0.79 }, // Llama 3.1 70B pricing
    features: ['multimodal', 'text_to_image', 'image_to_video', 'text_to_speech', 'high_quality_media']
  }
};

/**
 * API-based model loader for cloud LLM providers
 */
class APILoader extends BaseLoader {
  constructor(config = {}) {
    super(config);
    
    this.provider = config.provider || 'openai';
    this.apiKey = config.apiKey || process.env[`${this.provider.toUpperCase()}_API_KEY`];
    this.baseURL = config.baseURL || PROVIDER_CONFIGS[this.provider]?.baseURL;
    this.streamProcessor = new StreamProcessor();
    
    // Rate limiting
    this.rateLimiter = {
      requests: 0,
      resetTime: Date.now() + 60000,
      maxRequests: config.maxRequests || 100
    };
    
    // Response cache
    this.cache = new Map();
    this.cacheTimeout = config.cacheTimeout || 3600000; // 1 hour default
    
    // Cost tracking
    this.costs = {
      inputTokens: 0,
      outputTokens: 0,
      totalCost: 0
    };
    
    if (!this.apiKey) {
      logger.warn(`âš ï¸ No API key found for ${this.provider}. Set ${this.provider.toUpperCase()}_API_KEY environment variable.`);
    }
    
    logger.info(`ðŸŒ API Loader initialized for ${this.provider}`);
  }

  /**
   * Load/register an API model
   */
  async load(modelId, options = {}) {
    try {
      const providerConfig = PROVIDER_CONFIGS[this.provider];
      
      if (!providerConfig) {
        throw new Error(`Unsupported provider: ${this.provider}`);
      }
      
      if (!this.apiKey) {
        throw new Error(`API key required for ${this.provider}`);
      }
      
      // Validate model is supported
      if (this.provider !== 'openrouter' &amp;&amp; !providerConfig.models.includes(modelId)) {
        logger.warn(`Model ${modelId} may not be supported by ${this.provider}`);
      }
      
      // Create model wrapper
      const model = {
        id: `${this.provider}:${modelId}`,
        provider: this.provider,
        modelId: modelId,
        type: 'api',
        config: {
          ...providerConfig,
          ...options
        },
        metadata: {
          streaming: providerConfig.streaming,
          maxTokens: options.maxTokens || 4096,
          temperature: options.temperature || 0.7,
          costPerMillion: providerConfig.costPerMillion
        },
        loader: this
      };
      
      // Test connection
      if (options.testConnection !== false) {
        await this.testConnection(model);
      }
      
      this.model = model;
      this.loaded = true;
      
      logger.success(`âœ… Loaded API model: ${model.id}`);
      return model;
      
    } catch (error) {
      logger.error(`Failed to load API model: ${error.message}`);
      throw error;
    }
  }

  /**
   * Test API connection
   */
  async testConnection(model) {
    try {
      const testPrompt = 'Say "hello" in one word.';
      const response = await this.complete(testPrompt, {
        model: model,
        maxTokens: 10,
        stream: false
      });
      
      if (response) {
        logger.info(`âœ… API connection test successful for ${model.id}`);
        return true;
      }
    } catch (error) {
      logger.warn(`âš ï¸ API connection test failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Generate completion from API
   */
  async generate(prompt, options = {}) {
    if (!this.loaded || !this.model) {
      throw new Error('No model loaded');
    }
    
    return this.complete(prompt, { ...options, model: this.model });
  }

  /**
   * Complete prompt using API
   */
  async complete(prompt, options = {}) {
    const model = options.model || this.model;
    
    // Check cache
    const cacheKey = `${model.id}:${prompt}:${JSON.stringify(options)}`;
    if (this.cache.has(cacheKey)) {
      const cached = this.cache.get(cacheKey);
      if (Date.now() - cached.timestamp &lt; this.cacheTimeout) {
        logger.debug('Cache hit for prompt');
        return cached.response;
      }
    }
    
    // Rate limiting
    await this.checkRateLimit();
    
    // Build request based on provider
    const request = this.buildRequest(prompt, model, options);
    
    try {
      if (options.stream &amp;&amp; model.metadata.streaming) {
        return await this.streamCompletion(request, model, options);
      } else {
        return await this.standardCompletion(request, model, options);
      }
    } catch (error) {
      logger.error(`API request failed: ${error.message}`);
      
      // Retry logic
      if (options.retries > 0) {
        logger.info(`Retrying... (${options.retries} attempts left)`);
        await new Promise(resolve => setTimeout(resolve, 1000));
        return this.complete(prompt, { ...options, retries: options.retries - 1 });
      }
      
      throw error;
    }
  }

  /**
   * Build provider-specific request
   */
  buildRequest(prompt, model, options) {
    const baseRequest = {
      model: model.modelId,
      max_tokens: options.maxTokens || model.metadata.maxTokens,
      temperature: options.temperature || model.metadata.temperature,
      stream: options.stream || false
    };
    
    switch (this.provider) {
      case 'openai':
      case 'groq':
      case 'openrouter':
        return {
          ...baseRequest,
          messages: options.messages || [
            { role: 'user', content: prompt }
          ]
        };
        
      case 'anthropic':
        return {
          ...baseRequest,
          messages: options.messages || [
            { role: 'user', content: prompt }
          ],
          max_tokens: baseRequest.max_tokens
        };
        
      default:
        return baseRequest;
    }
  }

  /**
   * Standard (non-streaming) completion
   */
  async standardCompletion(request, model, options) {
    const endpoint = this.getEndpoint(model);
    const headers = PROVIDER_CONFIGS[this.provider].headers(this.apiKey);
    
    const response = await fetch(endpoint, {
      method: 'POST',
      headers,
      body: JSON.stringify(request)
    });
    
    if (!response.ok) {
      const error = await response.text();
      throw new Error(`API error (${response.status}): ${error}`);
    }
    
    const data = await response.json();
    const result = this.parseResponse(data, model);
    
    // Update costs
    this.updateCosts(result.usage);
    
    // Cache response
    this.cache.set(request.cacheKey, {
      response: result,
      timestamp: Date.now()
    });
    
    return result;
  }

  /**
   * Streaming completion
   */
  async streamCompletion(request, model, options) {
    const endpoint = this.getEndpoint(model);
    const headers = PROVIDER_CONFIGS[this.provider].headers(this.apiKey);
    
    const response = await fetch(endpoint, {
      method: 'POST',
      headers,
      body: JSON.stringify({ ...request, stream: true })
    });
    
    if (!response.ok) {
      const error = await response.text();
      throw new Error(`API error (${response.status}): ${error}`);
    }
    
    // Return async generator for streaming
    return this.streamProcessor.processStream(response.body, (chunk) => {
      return this.parseStreamChunk(chunk, model);
    });
  }

  /**
   * Parse provider-specific response
   */
  parseResponse(data, model) {
    let text, usage;
    
    switch (this.provider) {
      case 'openai':
      case 'groq':
      case 'openrouter':
        text = data.choices?.[0]?.message?.content || '';
        usage = data.usage;
        break;
        
      case 'anthropic':
        text = data.content?.[0]?.text || '';
        usage = {
          prompt_tokens: data.usage?.input_tokens,
          completion_tokens: data.usage?.output_tokens,
          total_tokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
        };
        break;
        
      default:
        text = data.text || data.response || '';
        usage = data.usage || {};
    }
    
    return {
      text,
      usage,
      model: model.id,
      provider: this.provider,
      timestamp: Date.now()
    };
  }

  /**
   * Parse streaming chunk
   */
  parseStreamChunk(chunk, model) {
    try {
      const lines = chunk.toString().split('\n').filter(line => line.trim());
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          if (data === '[DONE]') return null;
          
          const parsed = JSON.parse(data);
          
          switch (this.provider) {
            case 'openai':
            case 'groq':
            case 'openrouter':
              return parsed.choices?.[0]?.delta?.content || '';
              
            case 'anthropic':
              if (parsed.type === 'content_block_delta') {
                return parsed.delta?.text || '';
              }
              break;
          }
        }
      }
    } catch (error) {
      logger.debug('Error parsing stream chunk:', error);
    }
    
    return '';
  }

  /**
   * Get API endpoint for model
   */
  getEndpoint(model) {
    const base = this.baseURL || model.config.baseURL;
    
    switch (this.provider) {
      case 'openai':
      case 'groq':
      case 'openrouter':
        return `${base}/chat/completions`;
        
      case 'anthropic':
        return `${base}/messages`;
        
      default:
        return `${base}/completions`;
    }
  }

  /**
   * Check and update rate limits
   */
  async checkRateLimit() {
    if (Date.now() > this.rateLimiter.resetTime) {
      this.rateLimiter.requests = 0;
      this.rateLimiter.resetTime = Date.now() + 60000;
    }
    
    if (this.rateLimiter.requests >= this.rateLimiter.maxRequests) {
      const waitTime = this.rateLimiter.resetTime - Date.now();
      logger.warn(`Rate limit reached. Waiting ${waitTime}ms...`);
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }
    
    this.rateLimiter.requests++;
  }

  /**
   * Update cost tracking
   */
  updateCosts(usage) {
    if (!usage) return;
    
    const inputTokens = usage.prompt_tokens || usage.input_tokens || 0;
    const outputTokens = usage.completion_tokens || usage.output_tokens || 0;
    
    this.costs.inputTokens += inputTokens;
    this.costs.outputTokens += outputTokens;
    
    const costConfig = this.model.metadata.costPerMillion;
    if (costConfig) {
      const inputCost = (inputTokens / 1000000) * costConfig.input;
      const outputCost = (outputTokens / 1000000) * costConfig.output;
      this.costs.totalCost += inputCost + outputCost;
    }
  }

  /**
   * Get cost statistics
   */
  getCosts() {
    return {
      ...this.costs,
      provider: this.provider,
      model: this.model?.id
    };
  }

  /**
   * List available models for provider
   */
  async listModels() {
    const config = PROVIDER_CONFIGS[this.provider];
    
    if (this.provider === 'openrouter' &amp;&amp; this.apiKey) {
      // OpenRouter provides dynamic model list
      try {
        const response = await fetch(`${config.baseURL}/models`, {
          headers: config.headers(this.apiKey)
        });
        
        if (response.ok) {
          const data = await response.json();
          return data.data?.map(m => m.id) || config.models;
        }
      } catch (error) {
        logger.warn('Failed to fetch OpenRouter models:', error);
      }
    }
    
    return config?.models || [];
  }

  /**
   * Validate model configuration
   */
  validateModel(modelConfig) {
    if (!modelConfig.provider) {
      throw new Error('Provider is required for API models');
    }
    
    if (!PROVIDER_CONFIGS[modelConfig.provider]) {
      throw new Error(`Unsupported provider: ${modelConfig.provider}`);
    }
    
    return true;
  }

  /**
   * Unload model
   */
  async unload() {
    this.model = null;
    this.loaded = false;
    this.cache.clear();
    logger.info('API model unloaded');
  }

  /**
   * Get loader info
   */
  getInfo() {
    return {
      name: 'APILoader',
      version: '1.0.0',
      provider: this.provider,
      supportedProviders: Object.keys(PROVIDER_CONFIGS),
      hasApiKey: !!this.apiKey,
      costs: this.getCosts(),
      cacheSize: this.cache.size,
      rateLimitStatus: {
        requests: this.rateLimiter.requests,
        maxRequests: this.rateLimiter.maxRequests,
        resetIn: Math.max(0, this.rateLimiter.resetTime - Date.now())
      }
    };
  }
}

export default APILoader;</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="APILoader.html">APILoader</a></li><li><a href="AnthropicAdapter.html">AnthropicAdapter</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthManager.html">AuthManager</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="AzureOpenAIAdapter.html">AzureOpenAIAdapter</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BedrockAdapter.html">BedrockAdapter</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="CohereAdapter.html">CohereAdapter</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="DeepSeekAdapter.html">DeepSeekAdapter</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="ErrorHandler.html">ErrorHandler</a></li><li><a href="FireworksAdapter.html">FireworksAdapter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="GroqAdapter.html">GroqAdapter</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MistralAdapter.html">MistralAdapter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelError.html">ModelError</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelRegistry.html">ModelRegistry</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="NovitaAdapter.html">NovitaAdapter</a></li><li><a href="OpenAIAdapter.html">OpenAIAdapter</a></li><li><a href="OpenRouterAdapter.html">OpenRouterAdapter</a></li><li><a href="PerplexityAdapter.html">PerplexityAdapter</a></li><li><a href="Pipeline.html">Pipeline</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TogetherAdapter.html">TogetherAdapter</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="VertexAIAdapter.html">VertexAIAdapter</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#ADAPTER_REGISTRY">ADAPTER_REGISTRY</a></li><li><a href="global.html#API_KEY_PATTERNS">API_KEY_PATTERNS</a></li><li><a href="global.html#AUTH_TYPES">AUTH_TYPES</a></li><li><a href="global.html#AZURE_API_VERSIONS">AZURE_API_VERSIONS</a></li><li><a href="global.html#AZURE_OPENAI_MODELS">AZURE_OPENAI_MODELS</a></li><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BEDROCK_MODELS">BEDROCK_MODELS</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#CLAUDE_MODELS">CLAUDE_MODELS</a></li><li><a href="global.html#COHERE_MODELS">COHERE_MODELS</a></li><li><a href="global.html#COMPLIANCE_FEATURES">COMPLIANCE_FEATURES</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#DEEPSEEK_ENDPOINTS">DEEPSEEK_ENDPOINTS</a></li><li><a href="global.html#DEEPSEEK_MODELS">DEEPSEEK_MODELS</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#FIREWORKS_MODELS">FIREWORKS_MODELS</a></li><li><a href="global.html#GROQ_MODELS">GROQ_MODELS</a></li><li><a href="global.html#INPUT_TYPES">INPUT_TYPES</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#MISTRAL_MODELS">MISTRAL_MODELS</a></li><li><a href="global.html#MODEL_CATEGORIES">MODEL_CATEGORIES</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#NOVITA_ENDPOINTS">NOVITA_ENDPOINTS</a></li><li><a href="global.html#NOVITA_MODELS">NOVITA_MODELS</a></li><li><a href="global.html#OPENAI_MODELS">OPENAI_MODELS</a></li><li><a href="global.html#PERPLEXITY_MODELS">PERPLEXITY_MODELS</a></li><li><a href="global.html#POPULAR_MODELS">POPULAR_MODELS</a></li><li><a href="global.html#PROVIDER_AUTH_CONFIG">PROVIDER_AUTH_CONFIG</a></li><li><a href="global.html#PROVIDER_CATEGORIES">PROVIDER_CATEGORIES</a></li><li><a href="global.html#PROVIDER_CONFIGS">PROVIDER_CONFIGS</a></li><li><a href="global.html#PROVIDER_FEATURES">PROVIDER_FEATURES</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SAFETY_LEVELS">SAFETY_LEVELS</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TOGETHER_MODELS">TOGETHER_MODELS</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#VERTEX_AI_MODELS">VERTEX_AI_MODELS</a></li><li><a href="global.html#VERTEX_REGIONS">VERTEX_REGIONS</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#adjustTimeouts">adjustTimeouts</a></li><li><a href="global.html#attemptRecovery">attemptRecovery</a></li><li><a href="global.html#checkConnectivity">checkConnectivity</a></li><li><a href="global.html#clearCache">clearCache</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createAdapter">createAdapter</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#createMissingResources">createMissingResources</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#emergencyShutdown">emergencyShutdown</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#escalateError">escalateError</a></li><li><a href="global.html#executeRecovery">executeRecovery</a></li><li><a href="global.html#getAdapter">getAdapter</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#getProviderAuthType">getProviderAuthType</a></li><li><a href="global.html#getProviderInfo">getProviderInfo</a></li><li><a href="global.html#getProvidersByCategory">getProvidersByCategory</a></li><li><a href="global.html#getProvidersByFeature">getProvidersByFeature</a></li><li><a href="global.html#getStats">getStats</a></li><li><a href="global.html#getSupportedProviders">getSupportedProviders</a></li><li><a href="global.html#gracefulShutdown">gracefulShutdown</a></li><li><a href="global.html#handleCriticalError">handleCriticalError</a></li><li><a href="global.html#handleMemoryLeak">handleMemoryLeak</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#isProviderSupported">isProviderSupported</a></li><li><a href="global.html#logError">logError</a></li><li><a href="global.html#performHealthCheck">performHealthCheck</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#reinstallDependencies">reinstallDependencies</a></li><li><a href="global.html#reload">reload</a></li><li><a href="global.html#restartProcess">restartProcess</a></li><li><a href="global.html#retryConnection">retryConnection</a></li><li><a href="global.html#selectRecoveryStrategy">selectRecoveryStrategy</a></li><li><a href="global.html#setupHandlers">setupHandlers</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#softRestart">softRestart</a></li><li><a href="global.html#startHealthMonitoring">startHealthMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Wed Aug 20 2025 19:41:21 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
