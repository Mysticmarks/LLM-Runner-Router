<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: core/ErrorHandler.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: core/ErrorHandler.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * ðŸ›¡ï¸ System-Wide Error Handler with Self-Healing
 * Comprehensive error recovery and automatic healing system
 */

import { EventEmitter } from 'events';
import fs from 'fs/promises';
import path from 'path';
import { Logger } from '../utils/Logger.js';

const logger = new Logger('ErrorHandler');

/**
 * ðŸ›¡ï¸ System-Wide Error Handler with Self-Healing
 * Comprehensive error recovery and automatic healing system
 * 
 * @class ErrorHandler
 * @extends EventEmitter
 * @example
 * // Basic usage with default configuration
 * import ErrorHandler from './ErrorHandler.js';
 * 
 * const errorHandler = new ErrorHandler();
 * 
 * // Listen for recovery events
 * errorHandler.on('soft-restart', () => {
 *   console.log('System performing soft restart');
 * });
 * 
 * @example
 * // Advanced configuration with custom recovery strategies
 * const errorHandler = new ErrorHandler({
 *   maxRetries: 5,
 *   retryDelay: 2000,
 *   autoRestart: true,
 *   recoveryStrategies: {
 *     CUSTOM_ERROR: 'clearCache',
 *     API_TIMEOUT: 'retryConnection'
 *   }
 * });
 * 
 * @example
 * // Integration with existing application
 * class MyApp {
 *   constructor() {
 *     this.errorHandler = new ErrorHandler({
 *       memoryThreshold: 0.8,
 *       healthCheckInterval: 60000
 *     });
 *     
 *     // Handle application-specific recovery
 *     this.errorHandler.on('clear-cache', () => {
 *       this.clearApplicationCache();
 *     });
 *   }
 * }
 */
class ErrorHandler extends EventEmitter {
  constructor(config = {}) {
    super();
    
    this.config = {
      maxRetries: 3,
      retryDelay: 1000,
      healthCheckInterval: 30000,
      memoryThreshold: 0.9, // 90% memory usage threshold
      autoRestart: true,
      gracefulShutdownTimeout: 30000,
      errorLogPath: './logs/errors.json',
      recoveryStrategies: {
        ENOMEM: 'clearCache',
        ECONNREFUSED: 'retryConnection',
        ENOENT: 'createMissing',
        TIMEOUT: 'increaseTimeout',
        SEGFAULT: 'restartProcess'
      },
      ...config
    };
    
    this.errorCounts = new Map();
    this.recoveryAttempts = new Map();
    this.lastHealthCheck = Date.now();
    this.isRecovering = false;
    
    this.setupHandlers();
    this.startHealthMonitoring();
  }

  /**
   * Setup global error handlers
   * 
   * @method setupHandlers
   * @example
   * // Automatic setup on construction
   * const errorHandler = new ErrorHandler();
   * // Handlers are already set up
   * 
   * @example
   * // Manual setup after configuration changes
   * const errorHandler = new ErrorHandler({ autoRestart: false });
   * errorHandler.config.autoRestart = true;
   * errorHandler.setupHandlers(); // Re-setup with new config
   * 
   * @example
   * // Listen for specific error types
   * process.on('uncaughtException', (error) => {
   *   console.log('Caught by global handler:', error.message);
   * });
   * 
   * // Trigger test error
   * setTimeout(() => {
   *   throw new Error('Test uncaught exception');
   * }, 1000);
   */
  setupHandlers() {
    // Uncaught exceptions
    process.on('uncaughtException', (error) => {
      logger.error('ðŸš¨ Uncaught Exception:', error);
      this.handleCriticalError(error, 'uncaughtException');
    });

    // Unhandled promise rejections
    process.on('unhandledRejection', (reason, promise) => {
      logger.error('ðŸš¨ Unhandled Rejection:', reason);
      this.handleCriticalError(reason, 'unhandledRejection');
    });

    // Memory warnings
    process.on('warning', (warning) => {
      logger.warn('âš ï¸ Process Warning:', warning);
      if (warning.name === 'MaxListenersExceededWarning') {
        this.handleMemoryLeak();
      } else if (warning.name === 'TimeoutOverflowWarning') {
        this.emit('timeout-overflow', warning);
      } else if (warning.name === 'DeprecationWarning') {
        this.emit('deprecation-warning', warning);
      }
    });

    // Graceful shutdown signals
    process.on('SIGTERM', () => this.gracefulShutdown('SIGTERM'));
    process.on('SIGINT', () => this.gracefulShutdown('SIGINT'));
    process.on('SIGHUP', () => this.reload('SIGHUP'));

    // PM2 specific signals
    process.on('message', (msg) => {
      if (msg === 'shutdown') {
        this.gracefulShutdown('PM2');
      }
    });

    logger.info('âœ… Error handlers initialized');
  }

  /**
   * Handle memory leak detection
   * 
   * @method handleMemoryLeak
   * @example
   * // Automatic memory leak detection
   * process.on('warning', (warning) => {
   *   if (warning.name === 'MaxListenersExceededWarning') {
   *     console.log('Memory leak detected:', warning.message);
   *     errorHandler.handleMemoryLeak();
   *   }
   * });
   * 
   * @example
   * // Custom memory leak detection
   * setInterval(() => {
   *   const memUsage = process.memoryUsage();
   *   const usage = memUsage.heapUsed / memUsage.heapTotal;
   *   
   *   if (usage > 0.95) {
   *     console.warn('Potential memory leak detected');
   *     errorHandler.handleMemoryLeak();
   *   }
   * }, 30000);
   * 
   * @example
   * // Memory leak handling with diagnostics
   * class DiagnosticErrorHandler extends ErrorHandler {
   *   handleMemoryLeak() {
   *     console.log('Memory leak detected, running diagnostics');
   *     
   *     // Log memory usage details
   *     const usage = process.memoryUsage();
   *     console.log('Memory usage:', {
   *       rss: `${Math.round(usage.rss / 1024 / 1024)} MB`,
   *       heapTotal: `${Math.round(usage.heapTotal / 1024 / 1024)} MB`,
   *       heapUsed: `${Math.round(usage.heapUsed / 1024 / 1024)} MB`,
   *       external: `${Math.round(usage.external / 1024 / 1024)} MB`
   *     });
   *     
   *     // Trigger heap dump if available
   *     if (process.env.NODE_ENV === 'development') {
   *       this.createHeapDump();
   *     }
   *     
   *     // Attempt cleanup
   *     this.clearCache();
   *   }
   * }
   */
  handleMemoryLeak() {
    logger.warn('âš ï¸ Memory leak detected, initiating cleanup');
    
    // Log current memory usage
    const usage = process.memoryUsage();
    logger.warn('Memory usage:', {
      rss: `${Math.round(usage.rss / 1024 / 1024)} MB`,
      heapTotal: `${Math.round(usage.heapTotal / 1024 / 1024)} MB`,
      heapUsed: `${Math.round(usage.heapUsed / 1024 / 1024)} MB`,
      external: `${Math.round(usage.external / 1024 / 1024)} MB`
    });
    
    // Emit memory leak event
    this.emit('memory-leak-detected', usage);
    
    // Trigger cache clearing
    this.clearCache();
    
    // Schedule a health check
    setTimeout(() => {
      this.performHealthCheck();
    }, 5000);
  }

  /**
   * Handle critical errors with recovery
   * 
   * @method handleCriticalError
   * @param {Error} error - The error to handle
   * @param {string} source - Source of the error (e.g., 'uncaughtException')
   * @example
   * // Manual error handling
   * try {
   *   await riskyOperation();
   * } catch (error) {
   *   await errorHandler.handleCriticalError(error, 'riskyOperation');
   * }
   * 
   * @example
   * // Handling database connection errors
   * const dbError = new Error('Connection failed');
   * dbError.code = 'ECONNREFUSED';
   * await errorHandler.handleCriticalError(dbError, 'database');
   * 
   * @example
   * // Handling memory errors with custom recovery
   * const memError = new Error('Out of memory');
   * memError.code = 'ENOMEM';
   * 
   * // This will trigger 'clearCache' recovery strategy
   * await errorHandler.handleCriticalError(memError, 'memoryOperation');
   * 
   * @example
   * // Error frequency tracking and max retries
   * for (let i = 0; i &lt; 5; i++) {
   *   const error = new Error('Repeated error');
   *   error.code = 'TEST_ERROR';
   *   await errorHandler.handleCriticalError(error, 'testSource');
   *   // After maxRetries (default 3), will trigger emergency shutdown
   * }
   */
  async handleCriticalError(error, source) {
    // Log the error
    await this.logError(error, source);
    
    // Track error frequency
    const errorKey = `${error.code || error.name}_${source}`;
    const count = (this.errorCounts.get(errorKey) || 0) + 1;
    this.errorCounts.set(errorKey, count);
    
    // Check if we should attempt recovery
    if (count > this.config.maxRetries) {
      logger.error('âŒ Max retries exceeded, initiating emergency shutdown');
      return this.emergencyShutdown(error);
    }
    
    // Attempt recovery
    await this.attemptRecovery(error, source);
  }

  /**
   * Attempt to recover from error
   * 
   * @method attemptRecovery
   * @param {Error} error - The error to recover from
   * @param {string} source - Source of the error
   * @example
   * // Direct recovery attempt
   * const connectionError = new Error('Database timeout');
   * connectionError.code = 'ETIMEDOUT';
   * 
   * try {
   *   await errorHandler.attemptRecovery(connectionError, 'database');
   *   console.log('Recovery successful');
   * } catch (recoveryError) {
   *   console.log('Recovery failed:', recoveryError.message);
   * }
   * 
   * @example
   * // Recovery with event listening
   * errorHandler.on('retry-connection', ({ attempt, error }) => {
   *   console.log(`Retry attempt ${attempt} for:`, error.message);
   * });
   * 
   * const error = new Error('Network unreachable');
   * await errorHandler.attemptRecovery(error, 'network');
   * 
   * @example
   * // Preventing concurrent recoveries
   * const error1 = new Error('First error');
   * const error2 = new Error('Second error');
   * 
   * // Start first recovery
   * const recovery1 = errorHandler.attemptRecovery(error1, 'source1');
   * 
   * // Second recovery will be skipped due to isRecovering flag
   * const recovery2 = errorHandler.attemptRecovery(error2, 'source2');
   * 
   * await Promise.all([recovery1, recovery2]);
   */
  async attemptRecovery(error, source) {
    if (this.isRecovering) {
      logger.warn('â³ Recovery already in progress');
      return;
    }
    
    this.isRecovering = true;
    const recoveryKey = `${error.code || error.name}_${Date.now()}`;
    
    try {
      logger.info('ðŸ”§ Attempting recovery:', { error: error.code, source });
      
      // Select recovery strategy
      const strategy = this.selectRecoveryStrategy(error);
      
      // Execute recovery
      await this.executeRecovery(strategy, error);
      
      // Mark successful recovery
      this.recoveryAttempts.set(recoveryKey, 'success');
      logger.success('âœ… Recovery successful');
      
      // Reset error count after successful recovery
      const errorKey = `${error.code || error.name}_${source}`;
      this.errorCounts.set(errorKey, 0);
      
    } catch (recoveryError) {
      logger.error('âŒ Recovery failed:', recoveryError);
      this.recoveryAttempts.set(recoveryKey, 'failed');
      
      // Escalate if recovery fails
      await this.escalateError(error, recoveryError);
    } finally {
      this.isRecovering = false;
    }
  }

  /**
   * Select appropriate recovery strategy
   * 
   * @method selectRecoveryStrategy
   * @param {Error} error - The error to analyze
   * @returns {string} Recovery strategy name
   * @example
   * // Memory-related error
   * const memError = new Error('JavaScript heap out of memory');
   * const strategy = errorHandler.selectRecoveryStrategy(memError);
   * console.log(strategy); // 'clearCache'
   * 
   * @example
   * // Connection error with specific code
   * const connError = new Error('Connection refused');
   * connError.code = 'ECONNREFUSED';
   * const strategy = errorHandler.selectRecoveryStrategy(connError);
   * console.log(strategy); // 'retryConnection'
   * 
   * @example
   * // Missing module error
   * const moduleError = new Error('Cannot find module \'missing-package\'');
   * const strategy = errorHandler.selectRecoveryStrategy(moduleError);
   * console.log(strategy); // 'reinstallDependencies'
   * 
   * @example
   * // Custom strategy mapping
   * const errorHandler = new ErrorHandler({
   *   recoveryStrategies: {
   *     CUSTOM_CODE: 'customRecovery',
   *     API_LIMIT: 'waitAndRetry'
   *   }
   * });
   * 
   * const customError = new Error('Rate limit exceeded');
   * customError.code = 'API_LIMIT';
   * const strategy = errorHandler.selectRecoveryStrategy(customError);
   * console.log(strategy); // 'waitAndRetry'
   */
  selectRecoveryStrategy(error) {
    // Check predefined strategies
    if (error.code &amp;&amp; this.config.recoveryStrategies[error.code]) {
      return this.config.recoveryStrategies[error.code];
    }
    
    // Analyze error type
    if (error.message?.includes('memory')) {
      return 'clearCache';
    }
    
    if (error.message?.includes('ECONNREFUSED') || error.message?.includes('ETIMEDOUT')) {
      return 'retryConnection';
    }
    
    if (error.message?.includes('Cannot find module')) {
      return 'reinstallDependencies';
    }
    
    if (error.message?.includes('segmentation fault')) {
      return 'restartProcess';
    }
    
    // Default strategy
    return 'restart';
  }

  /**
   * Execute recovery strategy
   * 
   * @method executeRecovery
   * @param {string} strategy - Recovery strategy to execute
   * @param {Error} error - Original error that triggered recovery
   * @example
   * // Execute specific recovery strategy
   * const error = new Error('Memory leak detected');
   * await errorHandler.executeRecovery('clearCache', error);
   * 
   * @example
   * // Chain multiple recovery strategies
   * try {
   *   await errorHandler.executeRecovery('retryConnection', error);
   * } catch (firstRecoveryError) {
   *   await errorHandler.executeRecovery('restartProcess', error);
   * }
   * 
   * @example
   * // Listen for recovery events
   * errorHandler.on('clear-cache', () => {
   *   console.log('Application should clear its caches now');
   *   myApp.clearUserCache();
   *   myApp.clearApiCache();
   * });
   * 
   * await errorHandler.executeRecovery('clearCache', error);
   * 
   * @example
   * // Custom recovery implementation
   * class CustomErrorHandler extends ErrorHandler {
   *   async executeRecovery(strategy, error) {
   *     if (strategy === 'customStrategy') {
   *       console.log('Executing custom recovery');
   *       await this.customRecoveryMethod(error);
   *     } else {
   *       await super.executeRecovery(strategy, error);
   *     }
   *   }
   * }
   */
  async executeRecovery(strategy, error) {
    logger.info(`ðŸ”„ Executing recovery strategy: ${strategy}`);
    
    switch (strategy) {
      case 'clearCache':
        await this.clearCache();
        break;
        
      case 'retryConnection':
        await this.retryConnection(error);
        break;
        
      case 'createMissing':
        await this.createMissingResources(error);
        break;
        
      case 'increaseTimeout':
        await this.adjustTimeouts();
        break;
        
      case 'reinstallDependencies':
        await this.reinstallDependencies();
        break;
        
      case 'restartProcess':
        await this.restartProcess();
        break;
        
      case 'restart':
      default:
        await this.softRestart();
    }
  }

  /**
   * Recovery strategies - Clear cache and free memory
   * 
   * @method clearCache
   * @example
   * // Manual cache clearing
   * await errorHandler.clearCache();
   * console.log('System caches cleared');
   * 
   * @example
   * // Listen for cache clear events in your application
   * errorHandler.on('clear-cache', () => {
   *   // Clear application-specific caches
   *   userCache.clear();
   *   apiResponseCache.clear();
   *   imageCache.clear();
   * });
   * 
   * // Trigger cache clear
   * await errorHandler.clearCache();
   * 
   * @example
   * // Memory pressure response
   * const memoryUsage = process.memoryUsage();
   * const usage = memoryUsage.heapUsed / memoryUsage.heapTotal;
   * 
   * if (usage > 0.9) {
   *   console.log('High memory usage detected, clearing caches');
   *   await errorHandler.clearCache();
   * }
   * 
   * @example
   * // Force garbage collection with cache clear
   * // Run Node.js with: node --expose-gc app.js
   * if (global.gc) {
   *   await errorHandler.clearCache(); // This will trigger gc()
   *   console.log('Garbage collection completed');
   * }
   */
  async clearCache() {
    logger.info('ðŸ§¹ Clearing caches and freeing memory');
    
    // In ES modules, we can't clear require.cache
    // Instead, we'll clear any internal caches we maintain
    this.errorCounts.clear();
    this.recoveryAttempts.clear();
    
    // Force garbage collection if available
    if (global.gc) {
      global.gc();
    }
    
    // Clear application caches
    this.emit('clear-cache');
    
    // Wait for memory to stabilize
    await new Promise(resolve => setTimeout(resolve, 2000));
  }

  /**
   * Retry connection with exponential backoff
   * 
   * @method retryConnection
   * @param {Error} error - Original connection error
   * @example
   * // Database connection retry
   * const dbError = new Error('Connection timeout');
   * dbError.code = 'ETIMEDOUT';
   * 
   * try {
   *   await errorHandler.retryConnection(dbError);
   *   console.log('Database connection restored');
   * } catch (error) {
   *   console.log('Failed to restore connection after retries');
   * }
   * 
   * @example
   * // Monitor retry attempts
   * errorHandler.on('retry-connection', ({ attempt, error }) => {
   *   console.log(`Retry ${attempt}/3: ${error.message}`);
   *   
   *   // Show progress to user
   *   updateConnectionStatus(`Retrying connection... (${attempt}/3)`);
   * });
   * 
   * await errorHandler.retryConnection(connectionError);
   * 
   * @example
   * // Custom connectivity check integration
   * class DatabaseErrorHandler extends ErrorHandler {
   *   async checkConnectivity() {
   *     try {
   *       await database.ping();
   *       return true;
   *     } catch {
   *       return false;
   *     }
   *   }
   * }
   * 
   * @example
   * // API service retry with custom logic
   * errorHandler.on('retry-connection', async ({ attempt }) => {
   *   if (attempt === 1) {
   *     console.log('First retry: checking service status');
   *     const status = await checkAPIServiceStatus();
   *     console.log('Service status:', status);
   *   }
   * });
   */
  async retryConnection(error) {
    logger.info('ðŸ”Œ Retrying connection');
    
    for (let i = 0; i &lt; this.config.maxRetries; i++) {
      await new Promise(resolve => setTimeout(resolve, this.config.retryDelay * (i + 1)));
      
      // Emit retry event for components to handle
      this.emit('retry-connection', { attempt: i + 1, error });
      
      // Check if connection is restored
      const isConnected = await this.checkConnectivity();
      if (isConnected) {
        logger.success('âœ… Connection restored');
        return;
      }
    }
    
    throw new Error('Failed to restore connection');
  }

  /**
   * Create missing files and directories
   * 
   * @method createMissingResources
   * @param {Error} error - ENOENT error with path information
   * @example
   * // Handle missing log directory
   * const error = new Error("ENOENT: no such file or directory, open '/app/logs/error.log'");
   * await errorHandler.createMissingResources(error);
   * // Creates /app/logs/ directory and error.log file
   * 
   * @example
   * // Handle missing configuration file
   * try {
   *   const config = await fs.readFile('./config/app.json');
   * } catch (error) {
   *   if (error.code === 'ENOENT') {
   *     await errorHandler.createMissingResources(error);
   *     // Creates ./config/ directory and empty app.json file
   *     
   *     // Now write default configuration
   *     const defaultConfig = { port: 3000, env: 'development' };
   *     await fs.writeFile('./config/app.json', JSON.stringify(defaultConfig, null, 2));
   *   }
   * }
   * 
   * @example
   * // Automatic recovery from missing uploads directory
   * app.post('/upload', async (req, res) => {
   *   try {
   *     await saveFile('./uploads/user-file.jpg', req.file);
   *   } catch (error) {
   *     if (error.code === 'ENOENT') {
   *       await errorHandler.createMissingResources(error);
   *       // Retry the save operation
   *       await saveFile('./uploads/user-file.jpg', req.file);
   *     } else {
   *       throw error;
   *     }
   *   }
   * });
   * 
   * @example
   * // Bulk directory structure creation
   * const missingDirs = [
   *   './logs/application.log',
   *   './temp/cache.json',
   *   './data/users.db'
   * ];
   * 
   * for (const dir of missingDirs) {
   *   const error = new Error(`ENOENT: no such file or directory, open '${dir}'`);
   *   await errorHandler.createMissingResources(error);
   * }
   */
  async createMissingResources(error) {
    logger.info('ðŸ“ Creating missing resources');
    
    // Extract path from error
    const pathMatch = error.message.match(/ENOENT.*'([^']+)'/);
    if (pathMatch) {
      const missingPath = pathMatch[1];
      
      // Create directory if it's a directory
      if (missingPath.endsWith('/') || !path.extname(missingPath)) {
        await fs.mkdir(missingPath, { recursive: true });
        logger.info(`Created directory: ${missingPath}`);
      } else {
        // Create file with empty content
        await fs.mkdir(path.dirname(missingPath), { recursive: true });
        await fs.writeFile(missingPath, '');
        logger.info(`Created file: ${missingPath}`);
      }
    }
  }

  /**
   * Adjust system timeouts to handle slow operations
   * 
   * @method adjustTimeouts
   * @example
   * // Automatic timeout adjustment after timeouts
   * const timeoutError = new Error('Operation timed out');
   * timeoutError.code = 'TIMEOUT';
   * 
   * // This will trigger adjustTimeouts recovery
   * await errorHandler.handleCriticalError(timeoutError, 'slowOperation');
   * 
   * @example
   * // Listen for timeout adjustments in your application
   * errorHandler.on('adjust-timeouts', ({ multiplier }) => {
   *   console.log(`Adjusting timeouts by factor of ${multiplier}`);
   *   
   *   // Update application timeouts
   *   databaseTimeout *= multiplier;
   *   apiTimeout *= multiplier;
   *   fileOperationTimeout *= multiplier;
   *   
   *   console.log('New timeouts:', {
   *     database: databaseTimeout,
   *     api: apiTimeout,
   *     fileOperations: fileOperationTimeout
   *   });
   * });
   * 
   * @example
   * // Custom timeout adjustment logic
   * class AdaptiveErrorHandler extends ErrorHandler {
   *   async adjustTimeouts() {
   *     // Get current system load
   *     const load = await this.getSystemLoad();
   *     
   *     // Adjust multiplier based on load
   *     const multiplier = load > 0.8 ? 2.0 : 1.5;
   *     
   *     this.config.retryDelay *= multiplier;
   *     this.config.gracefulShutdownTimeout *= multiplier;
   *     
   *     this.emit('adjust-timeouts', { multiplier, load });
   *   }
   * }
   * 
   * @example
   * // Progressive timeout adjustment
   * let timeoutAdjustmentCount = 0;
   * 
   * errorHandler.on('adjust-timeouts', ({ multiplier }) => {
   *   timeoutAdjustmentCount++;
   *   
   *   if (timeoutAdjustmentCount > 3) {
   *     console.warn('Multiple timeout adjustments detected, investigating root cause');
   *     // Trigger deeper investigation
   *     performSystemDiagnostics();
   *   }
   * });
   */
  async adjustTimeouts() {
    logger.info('â° Adjusting timeouts');
    
    // Increase global timeouts
    this.config.retryDelay *= 1.5;
    this.config.gracefulShutdownTimeout *= 1.5;
    
    // Emit event for components to adjust their timeouts
    this.emit('adjust-timeouts', { multiplier: 1.5 });
  }

  /**
   * Trigger dependency reinstallation
   * 
   * @method reinstallDependencies
   * @example
   * // Automatic dependency reinstall for missing modules
   * try {
   *   require('some-missing-module');
   * } catch (error) {
   *   if (error.code === 'MODULE_NOT_FOUND') {
   *     await errorHandler.executeRecovery('reinstallDependencies', error);
   *   }
   * }
   * 
   * @example
   * // Listen for reinstall requirements
   * errorHandler.on('reinstall-required', () => {
   *   console.log('Dependencies need reinstallation');
   *   
   *   // Notify operations team
   *   sendAlert('Dependencies corrupted, reinstall required');
   *   
   *   // Log for audit trail
   *   logger.warn('Dependency reinstall triggered by error recovery');
   * });
   * 
   * @example
   * // PM2 integration for dependency reinstall
   * // In PM2 ecosystem file:
   * // {
   * //   "name": "app",
   * //   "script": "app.js",
   * //   "listen_timeout": 3000,
   * //   "kill_timeout": 5000
   * // }
   * 
   * errorHandler.on('reinstall-required', () => {
   *   if (process.send) {
   *     // PM2 will receive this and can trigger reinstall script
   *     process.send({
   *       type: 'reinstall',
   *       timestamp: Date.now(),
   *       reason: 'dependency_corruption'
   *     });
   *   }
   * });
   * 
   * @example
   * // Container/Docker environment handling
   * errorHandler.on('reinstall-required', async () => {
   *   if (process.env.CONTAINER_ENV) {
   *     console.log('Container environment detected');
   *     
   *     // Signal container orchestrator
   *     await fetch('http://orchestrator/api/restart-pod', {
   *       method: 'POST',
   *       body: JSON.stringify({
   *         podId: process.env.POD_ID,
   *         reason: 'dependency_reinstall_required'
   *       })
   *     });
   *   }
   * });
   */
  async reinstallDependencies() {
    logger.info('ðŸ“¦ Reinstalling dependencies');
    
    // This should be handled by PM2 or external script
    this.emit('reinstall-required');
    
    // Signal PM2 to restart with reinstall
    if (process.send) {
      process.send('reinstall');
    }
  }

  /**
   * Restart the entire process
   * 
   * @method restartProcess
   * @example
   * // Triggered by critical errors like segmentation faults
   * const criticalError = new Error('Segmentation fault detected');
   * criticalError.code = 'SEGFAULT';
   * 
   * // This will trigger process restart
   * await errorHandler.handleCriticalError(criticalError, 'nativeModule');
   * 
   * @example
   * // Manual process restart
   * if (memoryLeakDetected()) {
   *   console.log('Memory leak detected, restarting process');
   *   await errorHandler.restartProcess();
   * }
   * 
   * @example
   * // Process restart with PM2 integration
   * // PM2 will automatically restart the process
   * process.on('message', (msg) => {
   *   if (msg === 'restart') {
   *     console.log('PM2 restart signal received');
   *     // PM2 handles the actual restart
   *   }
   * });
   * 
   * @example
   * // Kubernetes pod restart
   * class KubernetesErrorHandler extends ErrorHandler {
   *   async restartProcess() {
   *     if (process.env.KUBERNETES_SERVICE_HOST) {
   *       console.log('Kubernetes environment detected');
   *       
   *       // Signal Kubernetes to restart pod
   *       await this.gracefulShutdown('kubernetes-restart');
   *       
   *       // Exit with code that triggers restart
   *       process.exit(1);
   *     } else {
   *       await super.restartProcess();
   *     }
   *   }
   * }
   * 
   * @example
   * // Process restart with state preservation
   * errorHandler.on('shutdown', async () => {
   *   // Save critical state before restart
   *   await saveApplicationState({
   *     activeConnections: getActiveConnections(),
   *     pendingTasks: getPendingTasks(),
   *     userSessions: getUserSessions(),
   *     timestamp: Date.now()
   *   });
   *   
   *   console.log('Application state saved for restart recovery');
   *   errorHandler.emit('shutdown-complete');
   * });
   */
  async restartProcess() {
    logger.info('ðŸ”„ Restarting process');
    
    // Clean shutdown
    await this.gracefulShutdown('error-recovery');
    
    // PM2 will handle the restart
    if (process.send) {
      process.send('restart');
    } else {
      // Self restart
      process.exit(1);
    }
  }

  /**
   * Perform soft restart without process termination
   * 
   * @method softRestart
   * @example
   * // Trigger soft restart manually
   * await errorHandler.softRestart();
   * console.log('Soft restart completed');
   * 
   * @example
   * // Application soft restart handling
   * errorHandler.on('soft-restart', async () => {
   *   console.log('Performing application soft restart');
   *   
   *   // Clear application state
   *   applicationState.reset();
   *   
   *   // Reinitialize connections
   *   await database.reconnect();
   *   await redis.reconnect();
   *   
   *   // Reload configuration
   *   config.reload();
   *   
   *   // Restart background services
   *   scheduler.restart();
   *   worker.restart();
   *   
   *   console.log('Soft restart completed successfully');
   * });
   * 
   * @example
   * // Web server soft restart
   * errorHandler.on('soft-restart', async () => {
   *   // Don't interrupt active connections
   *   server.pauseNewConnections();
   *   
   *   try {
   *     // Reload routes and middleware
   *     await server.reloadRoutes();
   *     await server.reloadMiddleware();
   *     
   *     // Update configuration
   *     await server.updateConfig();
   *     
   *     console.log('Web server soft restart completed');
   *   } finally {
   *     // Resume accepting new connections
   *     server.resumeNewConnections();
   *   }
   * });
   * 
   * @example
   * // Microservice soft restart with health checks
   * errorHandler.on('soft-restart', async () => {
   *   try {
   *     // Mark as unhealthy during restart
   *     healthCheck.setStatus('restarting');
   *     
   *     // Perform restart operations
   *     await reinitializeServices();
   *     await validateConnections();
   *     
   *     // Mark as healthy again
   *     healthCheck.setStatus('healthy');
   *     
   *     console.log('Microservice soft restart successful');
   *   } catch (error) {
   *     healthCheck.setStatus('unhealthy');
   *     throw error;
   *   }
   * });
   */
  async softRestart() {
    logger.info('â™»ï¸ Performing soft restart');
    
    // Emit soft restart event
    this.emit('soft-restart');
    
    // Wait for components to reset
    await new Promise(resolve => setTimeout(resolve, 3000));
  }

  /**
   * Start health monitoring system
   * 
   * @method startHealthMonitoring
   * @example
   * // Basic health monitoring
   * const errorHandler = new ErrorHandler({
   *   healthCheckInterval: 30000, // Check every 30 seconds
   *   memoryThreshold: 0.8 // Alert at 80% memory usage
   * });
   * 
   * errorHandler.startHealthMonitoring();
   * 
   * @example
   * // Listen for health events
   * errorHandler.on('health-check', (health) => {
   *   console.log('System Health:', {
   *     memory: `${(health.memory.heapUsed / 1024 / 1024).toFixed(2)} MB`,
   *     uptime: `${Math.floor(health.uptime / 60)} minutes`,
   *     errors: health.errors
   *   });
   * });
   * 
   * @example
   * // Custom health monitoring with alerts
   * errorHandler.on('health-check', (health) => {
   *   const memoryUsage = health.memory.heapUsed / health.memory.heapTotal;
   *   
   *   if (memoryUsage > 0.9) {
   *     sendAlert(`High memory usage: ${(memoryUsage * 100).toFixed(2)}%`);
   *   }
   *   
   *   if (health.errors > 5) {
   *     sendAlert(`High error rate: ${health.errors} errors detected`);
   *   }
   * });
   * 
   * @example
   * // Integration with external monitoring
   * errorHandler.on('high-error-rate', (errorCount) => {
   *   // Send to external monitoring service
   *   metrics.increment('error_rate', errorCount);
   *   alerts.send('high_error_rate', { count: errorCount });
   * });
   */
  startHealthMonitoring() {
    this.healthInterval = setInterval(async () => {
      await this.performHealthCheck();
    }, this.config.healthCheckInterval);
    
    logger.info('ðŸ¥ Health monitoring started');
  }

  /**
   * Perform comprehensive health check
   * 
   * @method performHealthCheck
   * @returns {Object} Health status object
   * @example
   * // Manual health check
   * const health = await errorHandler.performHealthCheck();
   * console.log('Current Health Status:', {
   *   memoryUsed: `${(health.memory.heapUsed / 1024 / 1024).toFixed(2)} MB`,
   *   uptime: `${Math.floor(health.uptime / 3600)} hours`,
   *   errorCount: health.errors
   * });
   * 
   * @example
   * // Scheduled health checks with custom intervals
   * setInterval(async () => {
   *   const health = await errorHandler.performHealthCheck();
   *   
   *   // Log to external service
   *   await logHealthMetrics(health);
   *   
   *   // Check thresholds
   *   if (health.memory.heapUsed > 500 * 1024 * 1024) { // 500MB
   *     console.warn('Memory usage high, considering restart');
   *   }
   * }, 60000);
   * 
   * @example
   * // Health check with custom metrics
   * class CustomErrorHandler extends ErrorHandler {
   *   async performHealthCheck() {
   *     const baseHealth = await super.performHealthCheck();
   *     
   *     // Add custom metrics
   *     return {
   *       ...baseHealth,
   *       database: await this.checkDatabaseHealth(),
   *       cache: await this.checkCacheHealth(),
   *       api: await this.checkAPIHealth()
   *     };
   *   }
   * }
   * 
   * @example
   * // React to health check results
   * errorHandler.on('health-check', async (health) => {
   *   const memUsage = health.memory.heapUsed / health.memory.heapTotal;
   *   
   *   if (memUsage > 0.85) {
   *     console.log('High memory usage, clearing caches');
   *     await errorHandler.clearCache();
   *   }
   *   
   *   // Store health metrics
   *   await database.insertHealthMetric({
   *     timestamp: health.timestamp,
   *     memory_usage: memUsage,
   *     error_count: health.errors,
   *     uptime: health.uptime
   *   });
   * });
   */
  async performHealthCheck() {
    const health = {
      timestamp: Date.now(),
      memory: process.memoryUsage(),
      cpu: process.cpuUsage(),
      uptime: process.uptime(),
      errors: this.errorCounts.size,
      recoveries: this.recoveryAttempts.size
    };
    
    // Check memory usage
    const memoryUsage = health.memory.heapUsed / health.memory.heapTotal;
    if (memoryUsage > this.config.memoryThreshold) {
      logger.warn(`âš ï¸ High memory usage: ${(memoryUsage * 100).toFixed(2)}%`);
      await this.clearCache();
    }
    
    // Check error rate
    const recentErrors = Array.from(this.errorCounts.values()).reduce((a, b) => a + b, 0);
    if (recentErrors > 10) {
      logger.warn(`âš ï¸ High error rate: ${recentErrors} errors`);
      this.emit('high-error-rate', recentErrors);
    }
    
    // Emit health status
    this.emit('health-check', health);
    
    // Log health status periodically
    if (Date.now() - this.lastHealthCheck > 300000) { // Every 5 minutes
      logger.info('ðŸ’š Health Check:', {
        memory: `${(memoryUsage * 100).toFixed(2)}%`,
        uptime: `${Math.floor(health.uptime / 60)} minutes`,
        errors: recentErrors
      });
      this.lastHealthCheck = Date.now();
    }
  }

  /**
   * Check network connectivity
   * 
   * @method checkConnectivity
   * @returns {boolean} True if connected, false otherwise
   * @example
   * // Basic connectivity check
   * const isConnected = await errorHandler.checkConnectivity();
   * if (isConnected) {
   *   console.log('Network is available');
   * } else {
   *   console.log('Network is down, switching to offline mode');
   * }
   * 
   * @example
   * // Use in retry logic
   * async function fetchData(url) {
   *   for (let attempt = 1; attempt &lt;= 3; attempt++) {
   *     try {
   *       return await fetch(url);
   *     } catch (error) {
   *       const isConnected = await errorHandler.checkConnectivity();
   *       if (!isConnected) {
   *         console.log('No internet connection, will retry when connected');
   *         await waitForConnection();
   *       }
   *     }
   *   }
   * }
   * 
   * @example
   * // Custom connectivity check
   * class CustomErrorHandler extends ErrorHandler {
   *   async checkConnectivity() {
   *     try {
   *       // Check your specific service
   *       const response = await fetch('https://api.myservice.com/health');
   *       return response.ok;
   *     } catch {
   *       // Fallback to default DNS check
   *       return await super.checkConnectivity();
   *     }
   *   }
   * }
   * 
   * @example
   * // Periodic connectivity monitoring
   * setInterval(async () => {
   *   const isConnected = await errorHandler.checkConnectivity();
   *   
   *   if (!isConnected) {
   *     console.log('Connection lost, pausing background tasks');
   *     pauseBackgroundJobs();
   *   } else {
   *     console.log('Connection restored, resuming background tasks');
   *     resumeBackgroundJobs();
   *   }
   * }, 30000);
   */
  async checkConnectivity() {
    try {
      // Try to resolve DNS
      const dns = await import('dns').then(m => m.promises);
      await dns.resolve4('google.com');
      return true;
    } catch {
      return false;
    }
  }

  /**
   * Perform graceful shutdown
   * 
   * @method gracefulShutdown
   * @param {string} signal - Shutdown signal (SIGTERM, SIGINT, etc.)
   * @example
   * // Manual graceful shutdown
   * process.on('SIGTERM', async () => {
   *   console.log('Received SIGTERM, shutting down gracefully');
   *   await errorHandler.gracefulShutdown('SIGTERM');
   * });
   * 
   * @example
   * // Application cleanup during shutdown
   * errorHandler.on('shutdown', (signal) => {
   *   console.log(`Shutdown initiated by ${signal}`);
   *   
   *   // Close database connections
   *   database.close();
   *   
   *   // Stop background jobs
   *   scheduler.stop();
   *   
   *   // Save in-progress work
   *   saveCurrentState();
   *   
   *   // Signal cleanup complete
   *   errorHandler.emit('shutdown-complete');
   * });
   * 
   * @example
   * // Timeout handling during shutdown
   * errorHandler.on('shutdown', async (signal) => {
   *   console.log('Starting cleanup...');
   *   
   *   try {
   *     // Perform cleanup with its own timeout
   *     await Promise.race([
   *       performCleanup(),
   *       new Promise((_, reject) => 
   *         setTimeout(() => reject(new Error('Cleanup timeout')), 25000)
   *       )
   *     ]);
   *     
   *     console.log('Cleanup completed successfully');
   *   } catch (error) {
   *     console.log('Cleanup timed out or failed:', error.message);
   *   } finally {
   *     errorHandler.emit('shutdown-complete');
   *   }
   * });
   * 
   * @example
   * // Docker/Kubernetes integration
   * // Graceful shutdown with health check endpoint
   * let isShuttingDown = false;
   * 
   * errorHandler.on('shutdown', () => {
   *   isShuttingDown = true;
   * });
   * 
   * app.get('/health', (req, res) => {
   *   if (isShuttingDown) {
   *     res.status(503).json({ status: 'shutting down' });
   *   } else {
   *     res.json({ status: 'healthy' });
   *   }
   * });
   */
  async gracefulShutdown(signal) {
    logger.info(`ðŸ›‘ Graceful shutdown initiated (${signal})`);
    
    // Stop health monitoring
    if (this.healthInterval) {
      clearInterval(this.healthInterval);
    }
    
    // Emit shutdown event
    this.emit('shutdown', signal);
    
    // Wait for cleanup with timeout
    const shutdownPromise = new Promise(resolve => {
      this.once('shutdown-complete', resolve);
    });
    
    const timeoutPromise = new Promise(resolve => {
      setTimeout(resolve, this.config.gracefulShutdownTimeout);
    });
    
    await Promise.race([shutdownPromise, timeoutPromise]);
    
    logger.info('ðŸ‘‹ Shutdown complete');
    process.exit(0);
  }

  /**
   * Perform emergency shutdown
   * 
   * @method emergencyShutdown
   * @param {Error} error - Critical error that triggered emergency shutdown
   * @example
   * // Triggered automatically after max retries
   * const criticalError = new Error('System corruption detected');
   * 
   * // This will trigger emergency shutdown after max retries
   * for (let i = 0; i &lt;= 3; i++) {
   *   await errorHandler.handleCriticalError(criticalError, 'corruption');
   * }
   * // Emergency shutdown initiated
   * 
   * @example
   * // Manual emergency shutdown
   * try {
   *   await performCriticalOperation();
   * } catch (error) {
   *   if (error.message.includes('security breach')) {
   *     console.log('Security breach detected, emergency shutdown');
   *     await errorHandler.emergencyShutdown(error);
   *   }
   * }
   * 
   * @example
   * // Emergency shutdown with custom logging
   * class SecurityErrorHandler extends ErrorHandler {
   *   async emergencyShutdown(error) {
   *     // Custom security logging
   *     await this.logSecurityIncident(error);
   *     
   *     // Notify security team
   *     await this.notifySecurityTeam(error);
   *     
   *     // Call parent emergency shutdown
   *     await super.emergencyShutdown(error);
   *   }
   * }
   * 
   * @example
   * // Monitor for emergency conditions
   * errorHandler.on('health-check', (health) => {
   *   const memUsage = health.memory.heapUsed / health.memory.heapTotal;
   *   
   *   // Emergency threshold reached
   *   if (memUsage > 0.98) {
   *     const error = new Error('Critical memory exhaustion');
   *     error.code = 'MEM_CRITICAL';
   *     errorHandler.emergencyShutdown(error);
   *   }
   * });
   */
  async emergencyShutdown(error) {
    logger.error('ðŸš¨ EMERGENCY SHUTDOWN', error);
    
    // Log critical error
    await this.logError(error, 'emergency');
    
    // Force exit
    process.exit(1);
  }

  /**
   * Reload configuration and reset state
   * 
   * @method reload
   * @param {string} signal - Signal that triggered reload (SIGHUP, etc.)
   * @example
   * // Manual configuration reload
   * process.on('SIGHUP', () => {
   *   console.log('Received SIGHUP, reloading configuration');
   *   errorHandler.reload('SIGHUP');
   * });
   * 
   * @example
   * // Application configuration reload
   * errorHandler.on('reload', async () => {
   *   console.log('Reloading application configuration');
   *   
   *   // Reload configuration files
   *   const newConfig = await loadConfiguration();
   *   app.updateConfig(newConfig);
   *   
   *   // Reinitialize connections with new settings
   *   await database.reconnect(newConfig.database);
   *   
   *   // Clear caches that might contain old config
   *   cache.clear();
   *   
   *   console.log('Configuration reload complete');
   * });
   * 
   * @example
   * // Hot reload with validation
   * errorHandler.on('reload', async () => {
   *   try {
   *     console.log('Validating new configuration...');
   *     const newConfig = await validateAndLoadConfig();
   *     
   *     console.log('Applying new configuration...');
   *     await applyConfiguration(newConfig);
   *     
   *     console.log('Configuration reload successful');
   *   } catch (error) {
   *     console.error('Configuration reload failed:', error);
   *     // Keep running with old configuration
   *   }
   * });
   * 
   * @example
   * // Zero-downtime reload for web servers
   * errorHandler.on('reload', async () => {
   *   console.log('Performing zero-downtime reload');
   *   
   *   // Update server configuration without stopping
   *   server.updateMiddleware(await loadMiddleware());
   *   server.updateRoutes(await loadRoutes());
   *   
   *   // Refresh SSL certificates if needed
   *   const certs = await loadSSLCertificates();
   *   if (certs.updated) {
   *     server.updateSSL(certs);
   *   }
   *   
   *   console.log('Zero-downtime reload complete');
   * });
   */
  async reload(signal) {
    logger.info(`ðŸ”„ Reloading configuration (${signal})`);
    
    // Emit reload event
    this.emit('reload');
    
    // Clear error counts
    this.errorCounts.clear();
    
    logger.success('âœ… Configuration reloaded');
  }

  /**
   * Log error to persistent storage
   * 
   * @method logError
   * @param {Error} error - Error to log
   * @param {string} source - Source/context of the error
   * @example
   * // Manual error logging
   * try {
   *   await performRiskyOperation();
   * } catch (error) {
   *   await errorHandler.logError(error, 'riskyOperation');
   *   // Error is now persisted to logs/errors.json
   * }
   * 
   * @example
   * // Custom error logging with additional context
   * class ContextualErrorHandler extends ErrorHandler {
   *   async logError(error, source, context = {}) {
   *     // Add request ID, user ID, etc.
   *     const enhancedError = {
   *       ...error,
   *       context: {
   *         requestId: context.requestId,
   *         userId: context.userId,
   *         userAgent: context.userAgent,
   *         timestamp: new Date().toISOString()
   *       }
   *     };
   *     
   *     await super.logError(enhancedError, source);
   *   }
   * }
   * 
   * @example
   * // Read error logs for analysis
   * import fs from 'fs/promises';
   * 
   * async function analyzeErrors() {
   *   try {
   *     const logData = await fs.readFile('./logs/errors.json', 'utf-8');
   *     const errors = JSON.parse(logData);
   *     
   *     // Find frequent errors
   *     const errorCounts = {};
   *     errors.forEach(log => {
   *       const key = `${log.error.name}_${log.source}`;
   *       errorCounts[key] = (errorCounts[key] || 0) + 1;
   *     });
   *     
   *     console.log('Most frequent errors:', errorCounts);
   *   } catch (error) {
   *     console.log('Could not read error logs:', error.message);
   *   }
   * }
   * 
   * @example
   * // Error log rotation and cleanup
   * class RotatingErrorHandler extends ErrorHandler {
   *   async logError(error, source) {
   *     await super.logError(error, source);
   *     
   *     // Check file size and rotate if needed
   *     const stats = await fs.stat(this.config.errorLogPath);
   *     const maxSize = 10 * 1024 * 1024; // 10MB
   *     
   *     if (stats.size > maxSize) {
   *       await this.rotateErrorLog();
   *     }
   *   }
   *   
   *   async rotateErrorLog() {
   *     const timestamp = new Date().toISOString().slice(0, 10);
   *     const backupPath = `./logs/errors-${timestamp}.json`;
   *     
   *     await fs.rename(this.config.errorLogPath, backupPath);
   *     console.log(`Error log rotated to ${backupPath}`);
   *   }
   * }
   */
  async logError(error, source) {
    const errorLog = {
      timestamp: new Date().toISOString(),
      source,
      error: {
        name: error.name,
        message: error.message,
        code: error.code,
        stack: error.stack
      },
      system: {
        memory: process.memoryUsage(),
        uptime: process.uptime(),
        platform: process.platform,
        node: process.version
      }
    };
    
    try {
      // Ensure log directory exists
      await fs.mkdir(path.dirname(this.config.errorLogPath), { recursive: true });
      
      // Read existing logs
      let logs = [];
      try {
        const data = await fs.readFile(this.config.errorLogPath, 'utf-8');
        logs = JSON.parse(data);
      } catch {
        // File doesn't exist or is invalid
      }
      
      // Add new log
      logs.push(errorLog);
      
      // Keep only last 1000 entries
      if (logs.length > 1000) {
        logs = logs.slice(-1000);
      }
      
      // Write back
      await fs.writeFile(this.config.errorLogPath, JSON.stringify(logs, null, 2));
    } catch (logError) {
      logger.error('Failed to write error log:', logError);
    }
  }

  /**
   * Escalate error to external monitoring systems
   * 
   * @method escalateError
   * @param {Error} originalError - The original error that occurred
   * @param {Error} recoveryError - Error that occurred during recovery attempt
   * @example
   * // Automatic escalation after recovery failure
   * try {
   *   await errorHandler.attemptRecovery(dbError, 'database');
   * } catch (recoveryError) {
   *   // This will trigger escalation
   *   await errorHandler.escalateError(dbError, recoveryError);
   * }
   * 
   * @example
   * // Listen for escalation events
   * errorHandler.on('error-escalation', async ({ original, recovery, timestamp }) => {
   *   console.log('Error escalated:', {
   *     original: original.message,
   *     recovery: recovery.message,
   *     time: new Date(timestamp).toISOString()
   *   });
   *   
   *   // Send to external monitoring
   *   await sendToSlack({
   *     channel: '#alerts',
   *     text: `ðŸš¨ Error Recovery Failed\nOriginal: ${original.message}\nRecovery: ${recovery.message}`
   *   });
   *   
   *   // Send to PagerDuty
   *   await createPagerDutyIncident({
   *     title: 'Error Recovery Failure',
   *     description: `Failed to recover from ${original.message}`,
   *     severity: 'high'
   *   });
   * });
   * 
   * @example
   * // Custom escalation with context
   * class MonitoredErrorHandler extends ErrorHandler {
   *   async escalateError(originalError, recoveryError) {
   *     // Add system context
   *     const context = {
   *       memory: process.memoryUsage(),
   *       uptime: process.uptime(),
   *       platform: process.platform,
   *       nodeVersion: process.version,
   *       errorHistory: this.getRecentErrors()
   *     };
   *     
   *     // Send to monitoring service
   *     await this.sendToMonitoring({
   *       originalError,
   *       recoveryError,
   *       context,
   *       severity: this.calculateSeverity(originalError)
   *     });
   *     
   *     // Call parent escalation
   *     await super.escalateError(originalError, recoveryError);
   *   }
   * }
   * 
   * @example
   * // Multi-channel escalation
   * errorHandler.on('error-escalation', async (escalation) => {
   *   const { original, recovery } = escalation;
   *   
   *   // Determine escalation channels based on error type
   *   const channels = [];
   *   
   *   if (original.message.includes('database')) {
   *     channels.push('database-team');
   *   }
   *   
   *   if (original.message.includes('payment')) {
   *     channels.push('payment-team', 'on-call-engineer');
   *   }
   *   
   *   if (recovery.message.includes('memory')) {
   *     channels.push('infrastructure-team');
   *   }
   *   
   *   // Send alerts to appropriate teams
   *   await Promise.all(channels.map(channel => 
   *     sendAlert(channel, escalation)
   *   ));
   * });
   */
  async escalateError(originalError, recoveryError) {
    logger.error('ðŸš¨ Escalating error to external monitoring');
    
    // Emit escalation event
    this.emit('error-escalation', {
      original: originalError,
      recovery: recoveryError,
      timestamp: Date.now()
    });
    
    // Could send to external monitoring service
    // await this.sendToMonitoring(originalError, recoveryError);
  }

  /**
   * Get comprehensive error and recovery statistics
   * 
   * @method getStats
   * @returns {Object} Statistics object with error counts, recovery attempts, and system metrics
   * @example
   * // Basic statistics monitoring
   * const stats = errorHandler.getStats();
   * console.log('Error Handler Statistics:', {
   *   totalErrors: Object.values(stats.errorCounts).reduce((a, b) => a + b, 0),
   *   totalRecoveries: Object.keys(stats.recoveryAttempts).length,
   *   isRecovering: stats.isRecovering,
   *   uptime: `${Math.floor(stats.uptime / 3600)} hours`,
   *   memoryUsage: `${(stats.memory.heapUsed / 1024 / 1024).toFixed(2)} MB`
   * });
   * 
   * @example
   * // Periodic statistics logging
   * setInterval(() => {
   *   const stats = errorHandler.getStats();
   *   const errorTotal = Object.values(stats.errorCounts).reduce((a, b) => a + b, 0);
   *   
   *   if (errorTotal > 0) {
   *     console.log('Error Summary:', {
   *       errors: stats.errorCounts,
   *       recoveries: stats.recoveryAttempts,
   *       memoryMB: Math.round(stats.memory.heapUsed / 1024 / 1024)
   *     });
   *   }
   * }, 300000); // Every 5 minutes
   * 
   * @example
   * // Statistics-based alerting
   * function checkErrorThresholds() {
   *   const stats = errorHandler.getStats();
   *   const errorCounts = stats.errorCounts;
   *   
   *   // Check for error spikes
   *   Object.entries(errorCounts).forEach(([errorType, count]) => {
   *     if (count > 10) {
   *       console.warn(`High error count for ${errorType}: ${count}`);
   *       sendAlert(`Error spike detected: ${errorType} occurred ${count} times`);
   *     }
   *   });
   *   
   *   // Check memory usage
   *   const memUsage = stats.memory.heapUsed / stats.memory.heapTotal;
   *   if (memUsage > 0.9) {
   *     sendAlert(`High memory usage: ${(memUsage * 100).toFixed(2)}%`);
   *   }
   * }
   * 
   * @example
   * // Export statistics for external monitoring
   * app.get('/metrics', (req, res) => {
   *   const stats = errorHandler.getStats();
   *   
   *   // Convert to Prometheus format
   *   const metrics = [
   *     `# HELP error_total Total number of errors`,
   *     `# TYPE error_total counter`,
   *     ...Object.entries(stats.errorCounts).map(([type, count]) => 
   *       `error_total{type="${type}"} ${count}`
   *     ),
   *     ``,
   *     `# HELP memory_usage_bytes Memory usage in bytes`,
   *     `# TYPE memory_usage_bytes gauge`,
   *     `memory_usage_bytes ${stats.memory.heapUsed}`,
   *     ``,
   *     `# HELP uptime_seconds Process uptime in seconds`,
   *     `# TYPE uptime_seconds gauge`,
   *     `uptime_seconds ${stats.uptime}`
   *   ].join('\n');
   *   
   *   res.set('Content-Type', 'text/plain');
   *   res.send(metrics);
   * });
   * 
   * @example
   * // Dashboard data preparation
   * function prepareDashboardData() {
   *   const stats = errorHandler.getStats();
   *   
   *   return {
   *     systemHealth: {
   *       status: stats.isRecovering ? 'recovering' : 'healthy',
   *       uptime: formatUptime(stats.uptime),
   *       memory: {
   *         used: Math.round(stats.memory.heapUsed / 1024 / 1024),
   *         total: Math.round(stats.memory.heapTotal / 1024 / 1024),
   *         percentage: Math.round((stats.memory.heapUsed / stats.memory.heapTotal) * 100)
   *       }
   *     },
   *     errorSummary: {
   *       total: Object.values(stats.errorCounts).reduce((a, b) => a + b, 0),
   *       byType: stats.errorCounts,
   *       recentRecoveries: Object.keys(stats.recoveryAttempts).length
   *     }
   *   };
   * }
   */
  getStats() {
    return {
      errorCounts: Object.fromEntries(this.errorCounts),
      recoveryAttempts: Object.fromEntries(this.recoveryAttempts),
      isRecovering: this.isRecovering,
      uptime: process.uptime(),
      memory: process.memoryUsage()
    };
  }
}


export default ErrorHandler;
export { ErrorHandler };
</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="APILoader.html">APILoader</a></li><li><a href="AnthropicAdapter.html">AnthropicAdapter</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthManager.html">AuthManager</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="AzureOpenAIAdapter.html">AzureOpenAIAdapter</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BedrockAdapter.html">BedrockAdapter</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="CohereAdapter.html">CohereAdapter</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="DeepSeekAdapter.html">DeepSeekAdapter</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="ErrorHandler.html">ErrorHandler</a></li><li><a href="FireworksAdapter.html">FireworksAdapter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="GroqAdapter.html">GroqAdapter</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MistralAdapter.html">MistralAdapter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelError.html">ModelError</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelRegistry.html">ModelRegistry</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="NovitaAdapter.html">NovitaAdapter</a></li><li><a href="OpenAIAdapter.html">OpenAIAdapter</a></li><li><a href="OpenRouterAdapter.html">OpenRouterAdapter</a></li><li><a href="PerplexityAdapter.html">PerplexityAdapter</a></li><li><a href="Pipeline.html">Pipeline</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TogetherAdapter.html">TogetherAdapter</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="VertexAIAdapter.html">VertexAIAdapter</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#ADAPTER_REGISTRY">ADAPTER_REGISTRY</a></li><li><a href="global.html#API_KEY_PATTERNS">API_KEY_PATTERNS</a></li><li><a href="global.html#AUTH_TYPES">AUTH_TYPES</a></li><li><a href="global.html#AZURE_API_VERSIONS">AZURE_API_VERSIONS</a></li><li><a href="global.html#AZURE_OPENAI_MODELS">AZURE_OPENAI_MODELS</a></li><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BEDROCK_MODELS">BEDROCK_MODELS</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#CLAUDE_MODELS">CLAUDE_MODELS</a></li><li><a href="global.html#COHERE_MODELS">COHERE_MODELS</a></li><li><a href="global.html#COMPLIANCE_FEATURES">COMPLIANCE_FEATURES</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#DEEPSEEK_ENDPOINTS">DEEPSEEK_ENDPOINTS</a></li><li><a href="global.html#DEEPSEEK_MODELS">DEEPSEEK_MODELS</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#FIREWORKS_MODELS">FIREWORKS_MODELS</a></li><li><a href="global.html#GROQ_MODELS">GROQ_MODELS</a></li><li><a href="global.html#INPUT_TYPES">INPUT_TYPES</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#MISTRAL_MODELS">MISTRAL_MODELS</a></li><li><a href="global.html#MODEL_CATEGORIES">MODEL_CATEGORIES</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#NOVITA_ENDPOINTS">NOVITA_ENDPOINTS</a></li><li><a href="global.html#NOVITA_MODELS">NOVITA_MODELS</a></li><li><a href="global.html#OPENAI_MODELS">OPENAI_MODELS</a></li><li><a href="global.html#PERPLEXITY_MODELS">PERPLEXITY_MODELS</a></li><li><a href="global.html#POPULAR_MODELS">POPULAR_MODELS</a></li><li><a href="global.html#PROVIDER_AUTH_CONFIG">PROVIDER_AUTH_CONFIG</a></li><li><a href="global.html#PROVIDER_CATEGORIES">PROVIDER_CATEGORIES</a></li><li><a href="global.html#PROVIDER_CONFIGS">PROVIDER_CONFIGS</a></li><li><a href="global.html#PROVIDER_FEATURES">PROVIDER_FEATURES</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SAFETY_LEVELS">SAFETY_LEVELS</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TOGETHER_MODELS">TOGETHER_MODELS</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#VERTEX_AI_MODELS">VERTEX_AI_MODELS</a></li><li><a href="global.html#VERTEX_REGIONS">VERTEX_REGIONS</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#adjustTimeouts">adjustTimeouts</a></li><li><a href="global.html#attemptRecovery">attemptRecovery</a></li><li><a href="global.html#checkConnectivity">checkConnectivity</a></li><li><a href="global.html#clearCache">clearCache</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createAdapter">createAdapter</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#createMissingResources">createMissingResources</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#emergencyShutdown">emergencyShutdown</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#escalateError">escalateError</a></li><li><a href="global.html#executeRecovery">executeRecovery</a></li><li><a href="global.html#getAdapter">getAdapter</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#getProviderAuthType">getProviderAuthType</a></li><li><a href="global.html#getProviderInfo">getProviderInfo</a></li><li><a href="global.html#getProvidersByCategory">getProvidersByCategory</a></li><li><a href="global.html#getProvidersByFeature">getProvidersByFeature</a></li><li><a href="global.html#getStats">getStats</a></li><li><a href="global.html#getSupportedProviders">getSupportedProviders</a></li><li><a href="global.html#gracefulShutdown">gracefulShutdown</a></li><li><a href="global.html#handleCriticalError">handleCriticalError</a></li><li><a href="global.html#handleMemoryLeak">handleMemoryLeak</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#isProviderSupported">isProviderSupported</a></li><li><a href="global.html#logError">logError</a></li><li><a href="global.html#performHealthCheck">performHealthCheck</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#reinstallDependencies">reinstallDependencies</a></li><li><a href="global.html#reload">reload</a></li><li><a href="global.html#restartProcess">restartProcess</a></li><li><a href="global.html#retryConnection">retryConnection</a></li><li><a href="global.html#selectRecoveryStrategy">selectRecoveryStrategy</a></li><li><a href="global.html#setupHandlers">setupHandlers</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#softRestart">softRestart</a></li><li><a href="global.html#startHealthMonitoring">startHealthMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Wed Aug 20 2025 19:41:21 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
