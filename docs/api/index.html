<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Home</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Home</h1>

    



    


    <h3> </h3>










    




    <section>
        <article><h1>ğŸ§  LLM Runner Router: Universal AI Model Orchestration System</h1>
<p><em>Where AI models transcend their formats, engines dance across dimensions, and intelligent inference becomes art</em></p>
<!-- SEO-optimized badges with alt text and better descriptions -->
<p><a href="https://echoaisystem.com"><img src="https://img.shields.io/badge/Built%20by-Echo%20AI%20Systems-blue" alt="Built by Echo AI Systems"></a>
<a href="LICENSE"><img src="https://img.shields.io/badge/License-MIT-green.svg" alt="MIT License"></a>
<a href="https://www.npmjs.com/package/llm-runner-router"><img src="https://img.shields.io/npm/v/llm-runner-router.svg" alt="NPM Package"></a>
<a href="https://nodejs.org/"><img src="https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg" alt="Node.js Version"></a>
<a href="docs/PERFORMANCE.md"><img src="https://img.shields.io/badge/WebGPU-Ready-orange" alt="WebGPU Ready"></a>
<a href="docs/MODEL_FORMATS.md"><img src="https://img.shields.io/badge/GGUF-Supported-purple" alt="GGUF Support"></a>
<a href="https://github.com/MCERQUA/LLM-Runner-Router"><img src="https://img.shields.io/badge/Open%20Source-Yes-brightgreen.svg" alt="Open Source"></a></p>
<hr>
<h2>ğŸ“‘ Table of Contents</h2>
<ul>
<li><a href="#what-is-llm-runner-router">ğŸŒŒ What Is LLM Runner Router?</a></li>
<li><a href="#core-features">âœ¨ Core Features</a></li>
<li><a href="#quick-start-guide">ğŸ® Quick Start Guide</a></li>
<li><a href="#documentation">ğŸ“š Documentation</a></li>
<li><a href="#live-demo">ğŸš€ Live Demo</a></li>
<li><a href="#installation">ğŸ’» Installation</a></li>
<li><a href="#architecture-overview">ğŸ—ï¸ Architecture Overview</a></li>
<li><a href="#performance-benchmarks">ğŸ“ˆ Performance Benchmarks</a></li>
<li><a href="#contributing">ğŸ¤ Contributing</a></li>
<li><a href="#support--community">ğŸ“§ Support &amp; Community</a></li>
<li><a href="#license">ğŸ“„ License</a></li>
</ul>
<hr>
<h2>ğŸ“Š Project Status</h2>
<p><strong>Current Version</strong>: 1.0.0 | <strong>Development Stage</strong>: Beta | <strong>Last Updated</strong>: December 2024</p>
<h3>Implementation Progress</h3>
<ul>
<li>âœ… <strong>Core Systems</strong>: 90% complete (Router, Registry, Pipeline, Error Handling)</li>
<li>âœ… <strong>Model Loaders</strong>: 85% complete (10 of 12 loaders implemented)</li>
<li>âœ… <strong>Engines</strong>: 50% complete (WebGPU, WASM, EngineSelector)</li>
<li>âœ… <strong>Runtime Features</strong>: 75% complete (Memory, Cache, Streaming)</li>
<li>âœ… <strong>API Layer</strong>: 60% complete (REST + WebSocket)</li>
<li>ğŸš§ <strong>Production Readiness</strong>: 35% complete</li>
<li>ğŸ“š <strong>Documentation</strong>: 45% complete</li>
<li>ğŸ§ª <strong>Testing</strong>: 35% complete (Unit + Integration tests)</li>
</ul>
<h2>ğŸŒŒ What Is LLM Runner Router?</h2>
<p><strong>LLM Runner Router</strong> is a revolutionary <strong>universal AI model orchestration system</strong> that intelligently manages, routes, and optimizes inference across multiple language models. Unlike traditional model loaders, our system provides:</p>
<ul>
<li><strong>ğŸ”® Universal Format Support</strong>: Seamlessly load GGUF, ONNX, Safetensors, HuggingFace, and emerging model formats</li>
<li><strong>âš¡ Multi-Engine Architecture</strong>: WebGPU for GPU acceleration, WASM for universal compatibility, Node.js for server deployment</li>
<li><strong>ğŸ§­ Intelligent Model Routing</strong>: Automatically select optimal models based on quality, cost, speed, or custom strategies</li>
<li><strong>ğŸš€ Real-Time Streaming</strong>: Stream tokens in real-time with async generators and WebSocket support</li>
<li><strong>ğŸ’° Cost Optimization</strong>: Minimize inference costs while maximizing performance and quality</li>
<li><strong>ğŸ¯ Zero-Configuration</strong>: Works out of the box with intelligent defaults, customizable to enterprise needs</li>
</ul>
<p>Perfect for developers building AI applications, researchers comparing models, and enterprises deploying scalable AI solutions.</p>
<h2>âœ¨ Core Features</h2>
<h3>ğŸ”® Universal Model Format Support (10 Loaders Implemented)</h3>
<ul>
<li><strong>GGUF</strong>: Complete support for GGML/GGUF quantized models with automatic detection âœ…</li>
<li><strong>BitNet (1-bit LLMs)</strong>: Revolutionary 1.58-bit quantization for 55-82% energy reduction âœ…</li>
<li><strong>ONNX</strong>: Full ONNX Runtime integration for cross-platform inference âœ…</li>
<li><strong>Safetensors</strong>: Secure tensor storage with lazy loading and float16 support âœ…</li>
<li><strong>HuggingFace Hub</strong>: Direct integration with transformers.js and model downloading âœ…</li>
<li><strong>PyTorch</strong>: Native PyTorch model loading âœ…</li>
<li><strong>Binary</strong>: Optimized binary format support âœ…</li>
<li><strong>Custom Formats</strong>: Extensible loader architecture for proprietary formats</li>
</ul>
<h3>âš¡ Multi-Engine Runtime Architecture</h3>
<ul>
<li><strong>WebGPU Engine</strong>: GPU-accelerated inference in browsers and modern runtimes</li>
<li><strong>WASM Engine</strong>: Universal compatibility across all platforms and devices</li>
<li><strong>Node.js Engine</strong>: High-performance server-side inference with native bindings</li>
<li><strong>Edge Computing</strong>: Optimized for Cloudflare Workers, Deno Deploy, and edge functions</li>
</ul>
<h3>ğŸ§­ Intelligent Model Routing Strategies</h3>
<ul>
<li><strong>Quality-First</strong>: Route to highest-quality models for critical applications</li>
<li><strong>Cost-Optimized</strong>: Minimize costs while maintaining acceptable quality thresholds</li>
<li><strong>Speed-Priority</strong>: Ultra-low latency routing for real-time applications</li>
<li><strong>Balanced</strong>: Optimal balance of quality, cost, and performance</li>
<li><strong>Custom Strategies</strong>: Define your own routing logic with JavaScript functions</li>
<li><strong>Load Balancing</strong>: Distribute requests across multiple model instances</li>
</ul>
<h3>ğŸš€ Advanced Streaming &amp; Real-Time Features âœ…</h3>
<ul>
<li><strong>Token Streaming</strong>: Real-time token generation with async generators via StreamProcessor âœ…</li>
<li><strong>WebSocket Support</strong>: Full bi-directional streaming API implemented âœ…</li>
<li><strong>Server-Sent Events</strong>: HTTP streaming for web applications âœ…</li>
<li><strong>Chunk Processing</strong>: Efficient batching and backpressure handling âœ…</li>
<li><strong>Parallel Processing</strong>: Concurrent requests across multiple models âœ…</li>
</ul>
<h3>ğŸ§  Runtime Optimization Features (All âœ… Complete)</h3>
<ul>
<li><strong>Memory Manager</strong>: Advanced memory optimization with compression and swapping âœ…</li>
<li><strong>Cache Manager</strong>: Multi-tier caching (L1 memory, L2 disk, L3 distributed-ready) âœ…</li>
<li><strong>Stream Processor</strong>: Real-time streaming with batching and backpressure control âœ…</li>
<li><strong>Thread Pool</strong>: Worker thread management with auto-scaling and task distribution âœ…</li>
<li><strong>Model Ensemble</strong>: Multiple ensemble strategies (weighted, voting, stacking, boosting) âœ…</li>
<li><strong>Self-Healing</strong>: Automatic error recovery and model fallback âœ…</li>
</ul>
<h2>ğŸ® Quick Start Guide</h2>
<h3>ğŸ’» Installation</h3>
<h4>NPM Installation (Recommended)</h4>
<pre class="prettyprint source lang-bash"><code># Install via NPM
npm install llm-runner-router

# Or with Yarn
yarn add llm-runner-router

# Or with PNPM  
pnpm add llm-runner-router
</code></pre>
<h4>Development Installation</h4>
<pre class="prettyprint source lang-bash"><code># Clone the repository
git clone https://github.com/MCERQUA/LLM-Runner-Router.git
cd LLM-Runner-Router

# Install dependencies
npm install

# Launch the development server
npm start

# Run tests
npm test

# Build for production
npm run build
</code></pre>
<h3>âš¡ Basic Usage Example</h3>
<pre class="prettyprint source lang-javascript"><code>import { LLMRouter } from 'llm-runner-router';

// Initialize the router with intelligent defaults
const router = new LLMRouter({
  strategy: 'balanced',
  engines: ['webgpu', 'wasm'],
  models: {
    'microsoft/DialoGPT-small': { priority: 'speed' },
    'meta-llama/Llama-2-7b-hf': { priority: 'quality' }
  }
});

// Simple text completion
const response = await router.complete(&quot;Explain quantum computing in simple terms:&quot;);
console.log(response.text);

// Streaming responses
for await (const chunk of router.stream(&quot;Write a story about AI:&quot;)) {
  process.stdout.write(chunk.text);
}
</code></pre>
<h2>ğŸš€ Live Demo</h2>
<p>Experience LLM Runner Router in action:</p>
<p>ğŸ® <strong><a href="https://github.com/MCERQUA/LLM-Runner-Router/tree/main/public/chat">Try Interactive Demo</a></strong> - Real-time model routing with streaming responses</p>
<p>ğŸ“š <strong><a href="https://github.com/MCERQUA/LLM-Runner-Router/tree/main/public/enhanced-docs.html">Browse Documentation</a></strong> - Complete API reference and guides</p>
<h2>ğŸ“š Documentation</h2>
<h3>Core Documentation</h3>
<ul>
<li>ğŸ—ï¸ <strong><a href="docs/ARCHITECTURE.md">Architecture Guide</a></strong> - System design and components</li>
<li>ğŸ“– <strong><a href="docs/API_REFERENCE.md">API Reference</a></strong> - Complete API documentation</li>
<li>ğŸ”§ <strong><a href="docs/CONFIG_REFERENCE.md">Configuration</a></strong> - Configuration options and examples</li>
<li>âš¡ <strong><a href="docs/PERFORMANCE.md">Performance Guide</a></strong> - Optimization and benchmarking</li>
</ul>
<h3>Advanced Topics</h3>
<ul>
<li>ğŸ§­ <strong><a href="docs/ROUTING_STRATEGIES.md">Routing Strategies</a></strong> - Model selection and load balancing</li>
<li>ğŸ“¦ <strong><a href="docs/MODEL_FORMATS.md">Model Formats</a></strong> - Supported formats and loaders</li>
<li>ğŸš€ <strong><a href="docs/DEPLOYMENT.md">Deployment Guide</a></strong> - Production deployment strategies</li>
<li>ğŸ”’ <strong><a href="docs/SECURITY.md">Security</a></strong> - Security best practices</li>
</ul>
<h3>Examples &amp; Tutorials</h3>
<ul>
<li>ğŸ“‹ <strong><a href="docs/EXAMPLES.md">Basic Examples</a></strong> - Simple usage patterns</li>
<li>ğŸŒŠ <strong><a href="docs/examples/STREAMING.md">Streaming Guide</a></strong> - Real-time streaming implementation</li>
<li>ğŸ³ <strong><a href="docs/examples/DOCKER.md">Docker Deployment</a></strong> - Containerized deployments</li>
<li>ğŸ“Š <strong><a href="docs/examples/MONITORING.md">Monitoring Setup</a></strong> - Performance monitoring</li>
</ul>
<h3>Help &amp; Support</h3>
<ul>
<li>â“ <strong><a href="docs/FAQ.md">FAQ</a></strong> - Frequently asked questions</li>
<li>ğŸ”§ <strong><a href="docs/TROUBLESHOOTING.md">Troubleshooting</a></strong> - Common issues and solutions</li>
<li>ğŸ“– <strong><a href="docs/GLOSSARY.md">Glossary</a></strong> - Technical terminology</li>
</ul>
<h2>ğŸ­ Usage Examples (Where Magic Happens)</h2>
<h3>Simple Mode - For Mortals</h3>
<pre class="prettyprint source lang-javascript"><code>import { quick } from 'llm-runner-router';

// Just ask, and ye shall receive
const response = await quick(&quot;Explain quantum computing to a goldfish&quot;);
console.log(response.text);
</code></pre>
<h3>Advanced Mode - For Wizards</h3>
<pre class="prettyprint source lang-javascript"><code>import LLMRouter from 'llm-runner-router';

const router = new LLMRouter({
  strategy: 'quality-first',
  enableQuantumMode: true // (Not actually quantum, but sounds cool)
});

// Load multiple models
await router.load('huggingface:meta-llama/Llama-2-7b');
await router.load('local:./models/mistral-7b.gguf');
await router.load('bitnet:microsoft/BitNet-b1.58-2B-4T');

// Let the router choose the best model
const response = await router.advanced({
  prompt: &quot;Write a haiku about JavaScript&quot;,
  temperature: 0.8,
  maxTokens: 50,
  fallbacks: ['gpt-3.5', 'local-llama']
});
</code></pre>
<h3>Streaming Mode - For The Real-Time Addicts</h3>
<pre class="prettyprint source lang-javascript"><code>const stream = router.stream(&quot;Tell me a story about a debugging dragon&quot;);

for await (const token of stream) {
  process.stdout.write(token);
}
</code></pre>
<h3>Ensemble Mode - For The Overachievers</h3>
<pre class="prettyprint source lang-javascript"><code>const result = await router.ensemble([
  { model: 'gpt-4', weight: 0.5 },
  { model: 'claude', weight: 0.3 },
  { model: 'llama', weight: 0.2 }
], &quot;What is the meaning of life?&quot;);

// Get wisdom from multiple AI perspectives!
</code></pre>
<h2>ğŸ”‹ BitNet: 1-bit LLM Revolution</h2>
<p>LLM Runner Router now supports <strong>Microsoft BitNet</strong> - revolutionary 1.58-bit quantized models that deliver:</p>
<ul>
<li><strong>55-82% energy reduction</strong> compared to FP16 models</li>
<li><strong>1.37x-6.17x speedup</strong> on CPU inference</li>
<li><strong>Run 100B models on a single CPU</strong> at human reading speeds</li>
<li><strong>Lossless inference quality</strong> despite extreme quantization</li>
</ul>
<h3>BitNet Setup</h3>
<pre class="prettyprint source lang-bash"><code># Install prerequisites (CMake required)
sudo apt-get install cmake  # Ubuntu/Debian
brew install cmake          # macOS

# Setup BitNet integration
npm run setup:bitnet

# Download a model
cd temp/bitnet-repo
python3 setup_env.py --hf-repo microsoft/BitNet-b1.58-2B-4T --quant-type i2_s
</code></pre>
<h3>BitNet Usage</h3>
<pre class="prettyprint source lang-javascript"><code>// Load official Microsoft BitNet model
const bitnetModel = await router.load({
  source: 'microsoft/BitNet-b1.58-2B-4T',
  type: 'bitnet',
  quantType: 'i2_s',
  threads: 4
});

// Generate with 1-bit efficiency
const response = await router.generate('Explain neural networks', {
  modelId: bitnetModel.id,
  maxTokens: 200
});
</code></pre>
<h2>ğŸ“ˆ Performance Benchmarks</h2>
<p>LLM Runner Router delivers exceptional performance across all supported engines:</p>
<table>
<thead>
<tr>
<th>Engine</th>
<th>Model Format</th>
<th>Tokens/sec</th>
<th>First Token (ms)</th>
<th>Memory Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebGPU</td>
<td>GGUF Q4</td>
<td>125</td>
<td>45</td>
<td>2.1 GB</td>
</tr>
<tr>
<td>WASM</td>
<td>ONNX</td>
<td>85</td>
<td>120</td>
<td>1.8 GB</td>
</tr>
<tr>
<td>Node.js</td>
<td>Safetensors</td>
<td>200</td>
<td>30</td>
<td>3.2 GB</td>
</tr>
<tr>
<td>BitNet</td>
<td>1.58-bit</td>
<td>150</td>
<td>35</td>
<td>0.7 GB</td>
</tr>
</tbody>
</table>
<p><em>Benchmarks run on MacBook Pro M2, 16GB RAM. Results may vary based on hardware.</em></p>
<h2>â“ Frequently Asked Questions</h2>
<h3>What model formats does LLM Runner Router support?</h3>
<p>LLM Runner Router supports all major AI model formats including GGUF, BitNet (1-bit LLMs), ONNX, Safetensors, HuggingFace Hub models, and custom formats. Our universal loader architecture automatically detects and optimizes loading for each format.</p>
<h3>Can I use LLM Runner Router in the browser?</h3>
<p>Yes! LLM Runner Router is designed for universal deployment. Use WebGPU for GPU-accelerated browser inference or WASM for maximum compatibility across all browsers and devices.</p>
<h3>How does intelligent model routing work?</h3>
<p>Our routing system evaluates models based on your configured strategy (quality, cost, speed, or balanced) and automatically selects the optimal model for each request. Custom routing strategies can be defined with JavaScript functions.</p>
<h3>Is LLM Runner Router suitable for production use?</h3>
<p>Absolutely. LLM Runner Router includes enterprise-grade features like load balancing, failover handling, performance monitoring, and security best practices. See our <a href="docs/DEPLOYMENT.md">deployment guide</a> for production setup.</p>
<h3>What's the difference between engines?</h3>
<ul>
<li><strong>WebGPU</strong>: GPU-accelerated inference for maximum performance</li>
<li><strong>WASM</strong>: Universal compatibility across all platforms</li>
<li><strong>Node.js</strong>: Server-side inference with native performance optimizations</li>
</ul>
<h3>Can I use multiple models simultaneously?</h3>
<p>Yes! LLM Runner Router supports model ensemble techniques, A/B testing, and parallel inference across multiple models with intelligent request distribution.</p>
<h2>ğŸ—ï¸ Architecture Overview</h2>
<pre class="prettyprint source"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Your Application                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            LLM-Runner-Router                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Router    â”‚ Pipeline â”‚    Registry       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Engines (WebGPU, WASM, Node)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Loaders (GGUF, ONNX, Safetensors)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2>ğŸ¯ Routing Strategies</h2>
<p>Choose your destiny:</p>
<ul>
<li><strong>ğŸ† Quality First</strong>: Only the finest neural outputs shall pass</li>
<li><strong>ğŸ’µ Cost Optimized</strong>: Your accountant will love you</li>
<li><strong>âš¡ Speed Priority</strong>: Gotta go fast!</li>
<li><strong>âš–ï¸ Balanced</strong>: The zen master approach</li>
<li><strong>ğŸ² Random</strong>: Embrace chaos, trust the universe</li>
<li><strong>ğŸ”„ Round Robin</strong>: Everyone gets a turn</li>
<li><strong>ğŸ“Š Least Loaded</strong>: Fair distribution of neural labor</li>
</ul>
<h2>ğŸ› ï¸ Configuration</h2>
<pre class="prettyprint source lang-javascript"><code>{
  &quot;routingStrategy&quot;: &quot;balanced&quot;,
  &quot;maxModels&quot;: 100,
  &quot;enableCaching&quot;: true,
  &quot;quantization&quot;: &quot;dynamic&quot;,
  &quot;preferredEngine&quot;: &quot;webgpu&quot;,
  &quot;maxTokens&quot;: 4096,
  &quot;cosmicAlignment&quot;: true  // Optional but recommended
}
</code></pre>
<h2>ğŸ“Š Performance Metrics</h2>
<ul>
<li><strong>Model Load Time</strong>: &lt; 500ms âš¡</li>
<li><strong>First Token</strong>: &lt; 100ms ğŸš€</li>
<li><strong>Throughput</strong>: &gt; 100 tokens/sec ğŸ’¨</li>
<li><strong>Memory Usage</strong>: &lt; 50% of model size ğŸ§ </li>
<li><strong>Quantum Entanglement</strong>: Yes âœ¨</li>
</ul>
<h2>ğŸ”§ Advanced Features</h2>
<h3>Custom Model Loaders</h3>
<pre class="prettyprint source lang-javascript"><code>router.registerLoader('my-format', MyCustomLoader);
</code></pre>
<h3>Cost Optimization</h3>
<pre class="prettyprint source lang-javascript"><code>const budget = 0.10; // $0.10 per request
const models = router.optimizeForBudget(availableModels, budget);
</code></pre>
<h3>Quality Scoring</h3>
<pre class="prettyprint source lang-javascript"><code>const scores = await router.rankModelsByQuality(models, prompt);
</code></pre>
<h2>ğŸŒ Deployment Options</h2>
<ul>
<li><strong>Browser</strong>: Full client-side inference with WebGPU</li>
<li><strong>Node.js</strong>: Server-side with native bindings</li>
<li><strong>Edge</strong>: Cloudflare Workers, Deno Deploy</li>
<li><strong>Docker</strong>: Container-ready out of the box</li>
<li><strong>Kubernetes</strong>: Scale to infinity and beyond</li>
</ul>
<h2>ğŸ¤ Contributing</h2>
<p>We welcome contributions from all dimensions! Whether you're fixing bugs, adding features, or improving documentation, your quantum entanglement with this project is appreciated.</p>
<ol>
<li>Fork the repository (in this dimension)</li>
<li>Create your feature branch (<code>git checkout -b feature/quantum-enhancement</code>)</li>
<li>Commit with meaningful messages (<code>git commit -m 'Add quantum tunneling support'</code>)</li>
<li>Push to your branch (<code>git push origin feature/quantum-enhancement</code>)</li>
<li>Open a Pull Request (and hope it doesn't collapse the wave function)</li>
</ol>
<h2>ğŸ“œ License</h2>
<p>MIT License - Because sharing is caring, and AI should be for everyone.</p>
<h2>ğŸ™ Acknowledgments</h2>
<ul>
<li>The Quantum Field for probabilistic inspiration</li>
<li>Coffee for keeping us in a superposition of awake and asleep</li>
<li>You, for reading this far and joining our neural revolution</li>
</ul>
<h2>ğŸš€ What's Next?</h2>
<h3>Currently In Development</h3>
<ul>
<li>[x] ONNX Runtime integration âœ…</li>
<li>[x] Safetensors loader âœ…</li>
<li>[x] HuggingFace Hub integration âœ…</li>
<li>[x] Memory optimization system âœ…</li>
<li>[x] Multi-tier caching âœ…</li>
<li>[x] WebSocket streaming API âœ…</li>
<li>[x] Integration test suite âœ…</li>
</ul>
<h3>Upcoming Features</h3>
<ul>
<li>[ ] GraphQL API endpoint</li>
<li>[ ] gRPC interface for high-performance RPC</li>
<li>[ ] TensorFlow.js loader</li>
<li>[ ] Node Native Engine optimizations</li>
<li>[ ] Docker &amp; Kubernetes deployment configs</li>
<li>[ ] OpenTelemetry monitoring integration</li>
<li>[ ] TypeScript definitions</li>
<li>[ ] E2E test coverage</li>
<li>[ ] Production security hardening</li>
</ul>
<hr>
<p><strong>Built with ğŸ’™ and â˜• by Echo AI Systems</strong></p>
<p><em>&quot;Because every business deserves an AI brain, and every AI brain deserves a proper orchestration system&quot;</em></p>
<hr>
<h2>ğŸ“ Support</h2>
<ul>
<li><strong>Documentation</strong>: <a href="docs/ARCHITECTURE.md">docs/ARCHITECTURE.md</a></li>
<li><strong>Issues</strong>: <a href="https://github.com/echoaisystems/llm-runner-router/issues">GitHub Issues</a></li>
<li><strong>Email</strong>: echoaisystems@gmail.com</li>
<li><strong>Telepathy</strong>: Focus really hard on your question</li>
</ul>
<p>Remember: With great model power comes great computational responsibility. Use wisely! ğŸ§™â€â™‚ï¸</p></article>
    </section>






</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Sun Aug 17 2025 22:28:58 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>