<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: loaders/GGUFLoader.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: loaders/GGUFLoader.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * ðŸ§  GGUF Model Loader - Production Implementation
 * Real inference using node-llama-cpp v3
 */

import { ModelInterface } from '../core/ModelInterface.js';
import { Logger } from '../utils/Logger.js';
import path from 'path';
import fs from 'fs/promises';

// Make node-llama-cpp optional
let getLlama, LlamaChatSession;
try {
  const llamaCpp = await import('node-llama-cpp');
  getLlama = llamaCpp.getLlama;
  LlamaChatSession = llamaCpp.LlamaChatSession;
} catch (error) {
  console.warn('node-llama-cpp not available - GGUF models will not be functional');
}

const logger = new Logger('GGUFLoader');

/**
 * GGUF Model - Production implementation with real inference
 */
class GGUFModel extends ModelInterface {
  constructor(config) {
    super(config);
    this.format = 'gguf';
    this.source = config.source || config.path; // Store the model path
    this.model = null;
    this.context = null;
    this.session = null;
    this.llama = null;
    
    // CPU optimization settings
    this.maxThreads = parseInt(config.maxThreads || process.env.LLM_MAX_THREADS || 2);
    this.contextSize = parseInt(config.contextSize || process.env.LLM_CONTEXT_SIZE || 2048);
    this.batchSize = parseInt(config.batchSize || process.env.LLM_BATCH_SIZE || 8);
  }

  /**
   * Load the GGUF model with CPU optimization for VPS environments
   * 
   * @example
   * // Basic model loading
   * const model = new GGUFModel({ source: './models/llama-7b-q4_k_m.gguf' });
   * await model.load();
   * 
   * @example
   * // Load with custom CPU settings for VPS
   * const model = new GGUFModel({
   *   source: './models/mistral-7b-q4_k_m.gguf',
   *   maxThreads: 2,        // Limit threads for VPS
   *   contextSize: 2048,    // Smaller context for memory efficiency
   *   batchSize: 8          // Smaller batch size for CPU
   * });
   * await model.load();
   * 
   * @example
   * // Load with error handling
   * try {
   *   const model = new GGUFModel({ source: './models/model.gguf' });
   *   await model.load();
   *   console.log('âœ… Model loaded successfully');
   * } catch (error) {
   *   if (error.message.includes('node-llama-cpp')) {
   *     console.error('âŒ node-llama-cpp dependency missing');
   *   } else {
   *     console.error('âŒ Failed to load model:', error.message);
   *   }
   * }
   */
  async load() {
    if (this.loaded) return;
    
    logger.info(`ðŸ§  Loading GGUF model: ${this.name}`);
    logger.info(`  Source path: ${this.source}`);
    this.loading = true;
    
    try {
      // Check if node-llama-cpp is available
      if (!getLlama) {
        throw new Error('node-llama-cpp is not installed. GGUF models require node-llama-cpp to function.');
      }
      
      // Get llama instance
      this.llama = await getLlama();
      
      // Resolve model path - handle both relative and absolute paths
      const modelPath = path.isAbsolute(this.source) 
        ? this.source 
        : path.resolve(process.cwd(), this.source);
      
      logger.info(`  Resolved path: ${modelPath}`);
      
      // Check if file exists
      await fs.access(modelPath);
      
      // Load the model
      this.model = await this.llama.loadModel({
        modelPath: modelPath,
        gpuLayers: 0 // CPU only for now, can be configured
      });
      
      // Create context with CPU optimization for VPS
      // Limit threads to leave headroom for system processes
      const cpuCount = (await import('os')).default.cpus().length;
      const optimalThreads = Math.max(1, Math.min(cpuCount - 1, this.maxThreads)); // Use configured max threads
      
      logger.info(`  CPU optimization: ${cpuCount} CPUs detected, using ${optimalThreads} threads`);
      
      this.context = await this.model.createContext({
        contextSize: this.contextSize,
        threads: optimalThreads,
        batchSize: this.batchSize // Smaller batch size for lower CPU usage
      });
      
      // Create a chat session
      this.session = new LlamaChatSession({
        contextSequence: this.context.getSequence(),
        systemPrompt: "You are a helpful AI assistant."
      });
      
      this.loaded = true;
      this.loading = false;
      this.metrics.loadTime = Date.now();
      
      logger.success(`âœ… GGUF model loaded: ${this.name}`);
    } catch (error) {
      this.error = error;
      this.loading = false;
      logger.error(`Failed to load GGUF model: ${error.message}`);
      throw error;
    }
  }

  /**
   * Generate text response from a prompt with GGUF model
   * 
   * @param {string} prompt - Input text prompt
   * @param {object} options - Generation options
   * @returns {Promise&lt;object>} Generation result with text, tokens, and metrics
   * 
   * @example
   * // Simple text generation
   * const result = await model.generate('Hello, how are you today?');
   * console.log(result.text);      // Generated response
   * console.log(result.tokens);    // Number of tokens
   * console.log(result.latency);   // Generation time in ms
   * 
   * @example
   * // Generation with custom parameters
   * const result = await model.generate('Explain quantum computing', {
   *   maxTokens: 300,        // Longer response
   *   temperature: 0.8,      // More creative
   *   topK: 50,             // Consider top 50 tokens
   *   topP: 0.95,           // Nucleus sampling
   *   repeatPenalty: 1.1,   // Reduce repetition
   *   stopStrings: ['\n\n', 'End'] // Stop generation at these strings
   * });
   * 
   * @example
   * // Track performance metrics
   * const startTime = Date.now();
   * const result = await model.generate('Write a short story');
   * console.log(`Generated ${result.tokens} tokens in ${result.latency}ms`);
   * console.log(`Tokens/sec: ${(result.tokens / result.latency * 1000).toFixed(2)}`);
   */
  async generate(prompt, options = {}) {
    if (!this.loaded) await this.load();
    
    const startTime = Date.now();
    
    try {
      // Update metrics
      this.metrics.inferenceCount++;
      this.metrics.lastUsed = new Date();
      
      // Generate response
      const response = await this.session.prompt(prompt, {
        maxTokens: options.maxTokens || 500,
        temperature: options.temperature || 0.7,
        topK: options.topK || 40,
        topP: options.topP || 0.95,
        repeatPenalty: options.repeatPenalty || 1.1,
        stopStrings: options.stopStrings || []
      });
      
      // Calculate metrics
      const endTime = Date.now();
      const latency = endTime - startTime;
      const tokens = response.split(/\s+/).length; // Approximate token count
      
      this.metrics.totalTokens += tokens;
      this.metrics.avgLatency = 
        (this.metrics.avgLatency * (this.metrics.inferenceCount - 1) + latency) / 
        this.metrics.inferenceCount;
      
      return {
        text: response,
        tokens,
        latency,
        model: this.name
      };
    } catch (error) {
      logger.error(`Generation failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Stream text generation token by token
   * 
   * @param {string} prompt - Input text prompt
   * @param {object} options - Streaming options
   * @yields {string} Individual tokens as they're generated
   * 
   * @example
   * // Basic streaming
   * console.log('Streaming response:');
   * for await (const token of model.stream('Tell me about AI')) {
   *   process.stdout.write(token); // Real-time output
   * }
   * console.log('\n--- Streaming complete ---');
   * 
   * @example
   * // Streaming with options and token counting
   * let tokenCount = 0;
   * let fullResponse = '';
   * 
   * for await (const token of model.stream('Explain machine learning', {
   *   maxTokens: 200,
   *   temperature: 0.7,
   *   topP: 0.9
   * })) {
   *   fullResponse += token;
   *   tokenCount++;
   *   
   *   // Update UI or log progress
   *   if (tokenCount % 10 === 0) {
   *     console.log(`\n[${tokenCount} tokens generated so far...]`);
   *   }
   * }
   * 
   * console.log(`\nFinal response: ${fullResponse}`);
   * console.log(`Total tokens: ${tokenCount}`);
   * 
   * @example
   * // Streaming with error handling and timeout
   * const timeout = setTimeout(() => {
   *   console.log('Generation taking too long, may need to cancel');
   * }, 30000); // 30 second timeout
   * 
   * try {
   *   for await (const token of model.stream('Complex question here')) {
   *     process.stdout.write(token);
   *   }
   *   clearTimeout(timeout);
   * } catch (error) {
   *   clearTimeout(timeout);
   *   console.error('Streaming failed:', error.message);
   * }
   */
  async *stream(prompt, options = {}) {
    if (!this.loaded) await this.load();
    
    const startTime = Date.now();
    let totalTokens = 0;
    
    try {
      // Update metrics
      this.metrics.inferenceCount++;
      this.metrics.lastUsed = new Date();
      
      // Stream response tokens
      const stream = this.session.promptStream(prompt, {
        maxTokens: options.maxTokens || 500,
        temperature: options.temperature || 0.7,
        topK: options.topK || 40,
        topP: options.topP || 0.95,
        repeatPenalty: options.repeatPenalty || 1.1
      });
      
      for await (const chunk of stream) {
        totalTokens++;
        yield chunk;
      }
      
      // Update metrics
      const endTime = Date.now();
      const latency = endTime - startTime;
      
      this.metrics.totalTokens += totalTokens;
      this.metrics.avgLatency = 
        (this.metrics.avgLatency * (this.metrics.inferenceCount - 1) + latency) / 
        this.metrics.inferenceCount;
      
    } catch (error) {
      logger.error(`Streaming failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Properly dispose of GGUF model and free all resources
   * 
   * @example
   * // Clean unload with verification
   * if (model.loaded) {
   *   await model.unload();
   *   console.log('ðŸ“¦ Model unloaded successfully');
   * }
   * 
   * @example
   * // Unload multiple models safely
   * const models = [model1, model2, model3];
   * await Promise.all(models.map(async (model) => {
   *   try {
   *     await model.unload();
   *     console.log(`Model ${model.name} unloaded`);
   *   } catch (error) {
   *     console.error(`Failed to unload ${model.name}:`, error.message);
   *   }
   * }));
   * 
   * @example
   * // Unload with cleanup verification
   * const memBefore = process.memoryUsage().heapUsed;
   * await model.unload();
   * 
   * // Force garbage collection if available
   * if (global.gc) {
   *   global.gc();
   * }
   * 
   * const memAfter = process.memoryUsage().heapUsed;
   * const memFreed = (memBefore - memAfter) / 1024 / 1024;
   * console.log(`Memory freed: ${memFreed.toFixed(2)} MB`);
   */
  async unload() {
    try {
      if (this.session) {
        this.session = null;
      }
      if (this.context) {
        await this.context.dispose();
        this.context = null;
      }
      if (this.model) {
        await this.model.dispose();
        this.model = null;
      }
      this.loaded = false;
      logger.info(`ðŸ“¦ GGUF model unloaded: ${this.name}`);
    } catch (error) {
      logger.error(`Cleanup failed: ${error.message}`);
    }
  }

  // Override toJSON to include source
  toJSON() {
    const json = super.toJSON();
    json.source = this.source;
    json.path = this.source; // Include both for compatibility
    return json;
  }
}

/**
 * GGUF Loader - The quantum gateway to quantized models
 */
class GGUFLoader {
  static format = 'gguf';
  static extensions = ['.gguf', '.ggml', '.bin'];

  /**
   * Check if a file path can be loaded by the GGUF loader
   * 
   * @param {string} path - File path to check
   * @returns {Promise&lt;boolean>} Whether the file can be loaded
   * 
   * @example
   * // Check single file
   * if (await loader.canLoad('./models/llama-7b.gguf')) {
   *   console.log('âœ… File can be loaded');
   *   await loader.load({ source: './models/llama-7b.gguf' });
   * }
   * 
   * @example
   * // Validate multiple model files
   * const modelPaths = [
   *   './models/llama-7b.gguf',
   *   './models/mistral-7b.ggml',
   *   './models/old-model.bin',
   *   './models/not-a-model.txt'
   * ];
   * 
   * for (const path of modelPaths) {
   *   const canLoad = await loader.canLoad(path);
   *   console.log(`${path}: ${canLoad ? 'âœ…' : 'âŒ'}`);
   * }
   * 
   * @example
   * // Dynamic model selection
   * const availableModels = await fs.readdir('./models');
   * const loadableModels = [];
   * 
   * for (const file of availableModels) {
   *   const fullPath = `./models/${file}`;
   *   if (await loader.canLoad(fullPath)) {
   *     loadableModels.push(fullPath);
   *   }
   * }
   * 
   * console.log(`Found ${loadableModels.length} loadable GGUF models`);
   */
  async canLoad(path) {
    const ext = path.split('.').pop().toLowerCase();
    return GGUFLoader.extensions.includes(`.${ext}`);
  }

  /**
   * Load a GGUF model with automatic configuration detection
   * 
   * @param {object} spec - Model specification
   * @returns {Promise&lt;GGUFModel>} Loaded model instance
   * 
   * @example
   * // Load with minimal specification
   * const model = await loader.load({
   *   source: './models/llama-7b-q4_k_m.gguf'
   * });
   * 
   * @example
   * // Load with custom configuration
   * const model = await loader.load({
   *   id: 'my-custom-model',
   *   name: 'Custom Llama 7B',
   *   source: './models/llama-7b-q4_k_m.gguf',
   *   quantization: 'q4_k_m',
   *   context: 4096,
   *   immediate: true  // Load immediately
   * });
   * 
   * @example
   * // Load with VPS-optimized settings
   * const model = await loader.load({
   *   source: './models/mistral-7b.gguf',
   *   maxThreads: 2,     // Limit for VPS
   *   contextSize: 2048, // Smaller context
   *   batchSize: 8,      // CPU-friendly batch size
   *   immediate: false   // Defer loading until needed
   * });
   * 
   * // Load when ready to use
   * await model.load();
   * 
   * @example
   * // Batch loading multiple models
   * const specs = [
   *   { source: './models/llama-7b.gguf', name: 'Llama 7B' },
   *   { source: './models/mistral-7b.gguf', name: 'Mistral 7B' },
   *   { source: './models/codellama-7b.gguf', name: 'Code Llama 7B' }
   * ];
   * 
   * const models = await Promise.all(
   *   specs.map(spec => loader.load({ ...spec, immediate: false }))
   * );
   * 
   * console.log(`Loaded ${models.length} GGUF models`);
   */
  async load(spec) {
    logger.info(`ðŸ”® Loading GGUF model from: ${spec.source}`);
    
    // Detect model properties
    const config = await this.detectConfig(spec);
    
    // Create model instance
    const model = new GGUFModel(config);
    
    // Load if immediate loading requested
    if (spec.immediate !== false) {
      await model.load();
    }
    
    return model;
  }

  async detectConfig(spec) {
    // Parse model name and properties from path/spec
    const config = {
      id: spec.id || this.generateId(),
      name: spec.name || this.extractName(spec.source),
      source: spec.source,
      quantization: spec.quantization || this.detectQuantization(spec.source || ''),
      context: spec.context || 2048
    };
    
    // Try to detect architecture
    if (spec.source &amp;&amp; spec.source.includes('llama')) {
      config.architecture = { type: 'llama' };
    } else if (spec.source &amp;&amp; spec.source.includes('mistral')) {
      config.architecture = { type: 'mistral' };
    }
    
    return config;
  }

  extractName(source) {
    // Extract name from path
    if (!source) return 'unknown';
    const parts = source.split('/');
    const filename = parts[parts.length - 1];
    return filename.replace(/\.(gguf|ggml|bin)$/i, '');
  }

  detectQuantization(source) {
    // Detect quantization from filename
    if (!source) return 'q4_k_m';
    const quants = ['q4_0', 'q4_1', 'q4_k_m', 'q4_k_s', 'q5_0', 'q5_1', 'q5_k_m', 'q5_k_s', 'q8_0'];
    
    for (const q of quants) {
      if (source.includes(q)) return q;
    }
    
    return 'q4_k_m'; // default
  }

  async validate(model) {
    // Validate GGUF model
    if (!model.weights) {
      throw new Error('GGUF model missing weights');
    }
    
    if (!model.vocab) {
      throw new Error('GGUF model missing vocabulary');
    }
    
    return true;
  }

  async optimize(model, options) {
    // Optimize GGUF model
    if (options.quantization) {
      await this.requantize(model, options.quantization);
    }
    
    return model;
  }

  async requantize(model, level) {
    logger.info(`âš¡ Requantizing model to ${level}`);
    // Implementation would convert between quantization levels
    model.quantization = level;
  }

  generateId() {
    return `gguf_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Restore a GGUF model from saved data/configuration
   * 
   * @param {object} data - Saved model data
   * @returns {Promise&lt;GGUFModel>} Restored model instance
   * 
   * @example
   * // Restore from saved configuration
   * const savedData = {
   *   id: 'llama-7b-model',
   *   name: 'Llama 7B Q4',
   *   source: './models/llama-7b-q4_k_m.gguf',
   *   quantization: 'q4_k_m',
   *   format: 'gguf'
   * };
   * 
   * const model = await loader.fromData(savedData);
   * await model.load(); // Load the restored model
   * 
   * @example
   * // Restore from registry export
   * const registry = JSON.parse(fs.readFileSync('./model-registry.json'));
   * const restoredModels = [];
   * 
   * for (const modelData of registry.models) {
   *   if (modelData.format === 'gguf') {
   *     const model = await loader.fromData(modelData);
   *     restoredModels.push(model);
   *   }
   * }
   * 
   * console.log(`Restored ${restoredModels.length} GGUF models from registry`);
   * 
   * @example
   * // Restore with error handling
   * try {
   *   const model = await loader.fromData(corruptedData);
   *   await model.load();
   *   console.log('Model restored and loaded successfully');
   * } catch (error) {
   *   console.error('Failed to restore model:', error.message);
   *   // Fall back to fresh loading
   *   const freshModel = await loader.load({
   *     source: corruptedData.source || corruptedData.path
   *   });
   * }
   */
  async fromData(data) {
    // Restore model from saved data with source path
    logger.info('Creating model from data:', data);
    const modelConfig = {
      ...data,
      source: data.source || data.path, // Handle both source and path fields
      format: 'gguf'
    };
    logger.info('Model config:', modelConfig);
    const model = new GGUFModel(modelConfig);
    return model;
  }
}

export default GGUFLoader;

</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="APILoader.html">APILoader</a></li><li><a href="AnthropicAdapter.html">AnthropicAdapter</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthManager.html">AuthManager</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="AzureOpenAIAdapter.html">AzureOpenAIAdapter</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BedrockAdapter.html">BedrockAdapter</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="CohereAdapter.html">CohereAdapter</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="DeepSeekAdapter.html">DeepSeekAdapter</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="ErrorHandler.html">ErrorHandler</a></li><li><a href="FireworksAdapter.html">FireworksAdapter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="GroqAdapter.html">GroqAdapter</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MistralAdapter.html">MistralAdapter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelError.html">ModelError</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelRegistry.html">ModelRegistry</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="NovitaAdapter.html">NovitaAdapter</a></li><li><a href="OpenAIAdapter.html">OpenAIAdapter</a></li><li><a href="OpenRouterAdapter.html">OpenRouterAdapter</a></li><li><a href="PerplexityAdapter.html">PerplexityAdapter</a></li><li><a href="Pipeline.html">Pipeline</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TogetherAdapter.html">TogetherAdapter</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="VertexAIAdapter.html">VertexAIAdapter</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#ADAPTER_REGISTRY">ADAPTER_REGISTRY</a></li><li><a href="global.html#API_KEY_PATTERNS">API_KEY_PATTERNS</a></li><li><a href="global.html#AUTH_TYPES">AUTH_TYPES</a></li><li><a href="global.html#AZURE_API_VERSIONS">AZURE_API_VERSIONS</a></li><li><a href="global.html#AZURE_OPENAI_MODELS">AZURE_OPENAI_MODELS</a></li><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BEDROCK_MODELS">BEDROCK_MODELS</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#CLAUDE_MODELS">CLAUDE_MODELS</a></li><li><a href="global.html#COHERE_MODELS">COHERE_MODELS</a></li><li><a href="global.html#COMPLIANCE_FEATURES">COMPLIANCE_FEATURES</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#DEEPSEEK_ENDPOINTS">DEEPSEEK_ENDPOINTS</a></li><li><a href="global.html#DEEPSEEK_MODELS">DEEPSEEK_MODELS</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#FIREWORKS_MODELS">FIREWORKS_MODELS</a></li><li><a href="global.html#GROQ_MODELS">GROQ_MODELS</a></li><li><a href="global.html#INPUT_TYPES">INPUT_TYPES</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#MISTRAL_MODELS">MISTRAL_MODELS</a></li><li><a href="global.html#MODEL_CATEGORIES">MODEL_CATEGORIES</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#NOVITA_ENDPOINTS">NOVITA_ENDPOINTS</a></li><li><a href="global.html#NOVITA_MODELS">NOVITA_MODELS</a></li><li><a href="global.html#OPENAI_MODELS">OPENAI_MODELS</a></li><li><a href="global.html#PERPLEXITY_MODELS">PERPLEXITY_MODELS</a></li><li><a href="global.html#POPULAR_MODELS">POPULAR_MODELS</a></li><li><a href="global.html#PROVIDER_AUTH_CONFIG">PROVIDER_AUTH_CONFIG</a></li><li><a href="global.html#PROVIDER_CATEGORIES">PROVIDER_CATEGORIES</a></li><li><a href="global.html#PROVIDER_CONFIGS">PROVIDER_CONFIGS</a></li><li><a href="global.html#PROVIDER_FEATURES">PROVIDER_FEATURES</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SAFETY_LEVELS">SAFETY_LEVELS</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TOGETHER_MODELS">TOGETHER_MODELS</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#VERTEX_AI_MODELS">VERTEX_AI_MODELS</a></li><li><a href="global.html#VERTEX_REGIONS">VERTEX_REGIONS</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#adjustTimeouts">adjustTimeouts</a></li><li><a href="global.html#attemptRecovery">attemptRecovery</a></li><li><a href="global.html#checkConnectivity">checkConnectivity</a></li><li><a href="global.html#clearCache">clearCache</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createAdapter">createAdapter</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#createMissingResources">createMissingResources</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#emergencyShutdown">emergencyShutdown</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#escalateError">escalateError</a></li><li><a href="global.html#executeRecovery">executeRecovery</a></li><li><a href="global.html#getAdapter">getAdapter</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#getProviderAuthType">getProviderAuthType</a></li><li><a href="global.html#getProviderInfo">getProviderInfo</a></li><li><a href="global.html#getProvidersByCategory">getProvidersByCategory</a></li><li><a href="global.html#getProvidersByFeature">getProvidersByFeature</a></li><li><a href="global.html#getStats">getStats</a></li><li><a href="global.html#getSupportedProviders">getSupportedProviders</a></li><li><a href="global.html#gracefulShutdown">gracefulShutdown</a></li><li><a href="global.html#handleCriticalError">handleCriticalError</a></li><li><a href="global.html#handleMemoryLeak">handleMemoryLeak</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#isProviderSupported">isProviderSupported</a></li><li><a href="global.html#logError">logError</a></li><li><a href="global.html#performHealthCheck">performHealthCheck</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#reinstallDependencies">reinstallDependencies</a></li><li><a href="global.html#reload">reload</a></li><li><a href="global.html#restartProcess">restartProcess</a></li><li><a href="global.html#retryConnection">retryConnection</a></li><li><a href="global.html#selectRecoveryStrategy">selectRecoveryStrategy</a></li><li><a href="global.html#setupHandlers">setupHandlers</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#softRestart">softRestart</a></li><li><a href="global.html#startHealthMonitoring">startHealthMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Wed Aug 20 2025 19:41:21 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
