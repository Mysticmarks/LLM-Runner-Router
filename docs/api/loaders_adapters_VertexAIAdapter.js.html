<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: loaders/adapters/VertexAIAdapter.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: loaders/adapters/VertexAIAdapter.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * ðŸ¢ Google Vertex AI Adapter
 * Enterprise-grade adapter for Google Cloud Vertex AI models
 * Features: Gemini Pro/Ultra, PaLM 2, Codey, MLOps integration
 */

import APILoader from '../APILoader.js';
import { Logger } from '../../utils/Logger.js';
import { AuthManager } from '../../utils/AuthManager.js';

const logger = new Logger('VertexAIAdapter');

/**
 * Vertex AI model configurations
 */
const VERTEX_AI_MODELS = {
  // Gemini models
  'gemini-1.5-pro': {
    name: 'Gemini 1.5 Pro',
    contextWindow: 2097152, // 2M tokens
    maxOutput: 8192,
    cost: { input: 3.5, output: 10.5 },
    features: ['multimodal', 'long_context', 'code_generation', 'function_calling'],
    family: 'gemini',
    version: 'gemini-1.5-pro'
  },
  'gemini-1.0-pro': {
    name: 'Gemini 1.0 Pro',
    contextWindow: 32760,
    maxOutput: 2048,
    cost: { input: 0.5, output: 1.5 },
    features: ['multimodal', 'reasoning', 'code_generation'],
    family: 'gemini',
    version: 'gemini-1.0-pro'
  },
  'gemini-1.0-pro-vision': {
    name: 'Gemini 1.0 Pro Vision',
    contextWindow: 16384,
    maxOutput: 2048,
    cost: { input: 0.25, output: 0.5 },
    features: ['vision', 'image_understanding', 'multimodal'],
    family: 'gemini',
    version: 'gemini-1.0-pro-vision'
  },
  'gemini-1.0-ultra': {
    name: 'Gemini 1.0 Ultra',
    contextWindow: 32760,
    maxOutput: 2048,
    cost: { input: 20, output: 60 },
    features: ['reasoning', 'complex_tasks', 'multimodal'],
    family: 'gemini',
    version: 'gemini-1.0-ultra'
  },

  // PaLM 2 models
  'text-bison': {
    name: 'PaLM 2 Text Bison',
    contextWindow: 8196,
    maxOutput: 1024,
    cost: { input: 1.0, output: 1.0 },
    features: ['text_generation', 'general_purpose'],
    family: 'palm',
    version: 'text-bison@001'
  },
  'text-bison-32k': {
    name: 'PaLM 2 Text Bison 32K',
    contextWindow: 32000,
    maxOutput: 8192,
    cost: { input: 1.25, output: 1.25 },
    features: ['text_generation', 'long_context'],
    family: 'palm',
    version: 'text-bison-32k@002'
  },
  'chat-bison': {
    name: 'PaLM 2 Chat Bison',
    contextWindow: 4096,
    maxOutput: 1024,
    cost: { input: 0.5, output: 0.5 },
    features: ['conversational', 'dialogue'],
    family: 'palm',
    version: 'chat-bison@001'
  },
  'chat-bison-32k': {
    name: 'PaLM 2 Chat Bison 32K',
    contextWindow: 32000,
    maxOutput: 8192,
    cost: { input: 0.625, output: 0.625 },
    features: ['conversational', 'long_context'],
    family: 'palm',
    version: 'chat-bison-32k@002'
  },

  // Code models
  'code-bison': {
    name: 'Codey Code Bison',
    contextWindow: 6144,
    maxOutput: 1024,
    cost: { input: 0.5, output: 0.5 },
    features: ['code_generation', 'programming'],
    family: 'codey',
    version: 'code-bison@001'
  },
  'code-bison-32k': {
    name: 'Codey Code Bison 32K',
    contextWindow: 32000,
    maxOutput: 8192,
    cost: { input: 0.625, output: 0.625 },
    features: ['code_generation', 'long_context', 'programming'],
    family: 'codey',
    version: 'code-bison-32k@002'
  },
  'codechat-bison': {
    name: 'Codey Chat Bison',
    contextWindow: 6144,
    maxOutput: 1024,
    cost: { input: 0.5, output: 0.5 },
    features: ['code_chat', 'programming_assistance'],
    family: 'codey',
    version: 'codechat-bison@001'
  },
  'codechat-bison-32k': {
    name: 'Codey Chat Bison 32K',
    contextWindow: 32000,
    maxOutput: 8192,
    cost: { input: 0.625, output: 0.625 },
    features: ['code_chat', 'long_context', 'programming_assistance'],
    family: 'codey',
    version: 'codechat-bison-32k@002'
  },

  // Embedding models
  'textembedding-gecko': {
    name: 'Text Embedding Gecko',
    contextWindow: 3072,
    maxOutput: 768, // Embedding dimensions
    cost: { input: 0.1, output: 0 },
    features: ['embeddings', 'semantic_search'],
    family: 'embedding',
    version: 'textembedding-gecko@001',
    type: 'embedding'
  },
  'textembedding-gecko-multilingual': {
    name: 'Text Embedding Gecko Multilingual',
    contextWindow: 3072,
    maxOutput: 768,
    cost: { input: 0.1, output: 0 },
    features: ['embeddings', 'multilingual', 'semantic_search'],
    family: 'embedding',
    version: 'textembedding-gecko-multilingual@001',
    type: 'embedding'
  }
};

/**
 * Google Vertex AI regions
 */
const VERTEX_REGIONS = {
  'us-central1': 'Iowa',
  'us-east1': 'South Carolina',
  'us-east4': 'Northern Virginia',
  'us-west1': 'Oregon',
  'us-west4': 'Las Vegas',
  'europe-west1': 'Belgium',
  'europe-west4': 'Netherlands',
  'asia-southeast1': 'Singapore',
  'asia-northeast1': 'Tokyo'
};

/**
 * Google Vertex AI adapter with enterprise features
 */
class VertexAIAdapter extends APILoader {
  constructor(config = {}) {
    super({
      ...config,
      provider: 'vertex-ai'
    });

    // GCP-specific configuration
    this.projectId = config.projectId || process.env.GOOGLE_CLOUD_PROJECT;
    this.location = config.location || process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';
    this.credentials = config.credentials;
    this.keyFilename = config.keyFilename || process.env.GOOGLE_APPLICATION_CREDENTIALS;
    this.useDefaultCredentials = config.useDefaultCredentials || false;

    // Enterprise features
    this.enablePrivateEndpoint = config.enablePrivateEndpoint || false;
    this.encryptionKey = config.encryptionKey;
    this.network = config.network;

    this.authManager = new AuthManager();
    this.models = new Map();
    this.tokenCache = new Map();
    this.vertexClient = null;

    // Validate configuration
    this.validateConfig();

    // Initialize GCP client
    this.initializeClient();

    logger.info(`ðŸ¢ Google Vertex AI Adapter initialized (Project: ${this.projectId}, Location: ${this.location})`);
  }

  /**
   * Validate Vertex AI configuration
   */
  validateConfig() {
    if (!this.projectId) {
      throw new Error('Google Cloud Project ID is required');
    }

    if (!VERTEX_REGIONS[this.location]) {
      logger.warn(`Unknown Vertex AI region: ${this.location}. Proceeding anyway.`);
    }

    if (!this.keyFilename &amp;&amp; !this.credentials &amp;&amp; !this.useDefaultCredentials) {
      throw new Error('Google Cloud credentials required (keyFilename, credentials, or useDefaultCredentials)');
    }
  }

  /**
   * Initialize Google Cloud client
   */
  async initializeClient() {
    try {
      // In real implementation, would initialize Vertex AI client from Google Cloud SDK
      this.vertexClient = {
        projectId: this.projectId,
        location: this.location,
        credentials: this.credentials,
        keyFilename: this.keyFilename,
        initialized: true
      };

      logger.info('âœ… Google Vertex AI client initialized');
    } catch (error) {
      logger.error('Failed to initialize Google Vertex AI client:', error);
      throw error;
    }
  }

  /**
   * Get authentication headers
   */
  getHeaders() {
    return {
      'Content-Type': 'application/json',
      'User-Agent': 'LLM-Runner-Router/2.0.0',
      'Authorization': `Bearer ${this.getAccessToken()}`
    };
  }

  /**
   * Get Google Cloud access token
   */
  getAccessToken() {
    // In real implementation, would use Google Auth libraries
    const cached = this.tokenCache.get('gcp_token');
    
    if (cached &amp;&amp; Date.now() &lt; cached.expiresAt - 300000) { // 5 min buffer
      return cached.token;
    }

    // Simulate token refresh
    const token = 'simulated_gcp_access_token';
    this.tokenCache.set('gcp_token', {
      token,
      expiresAt: Date.now() + 3600000 // 1 hour
    });

    return token;
  }

  /**
   * Load Vertex AI model
   */
  async load(modelId, options = {}) {
    try {
      const modelConfig = VERTEX_AI_MODELS[modelId];
      
      if (!modelConfig) {
        logger.warn(`Unknown Vertex AI model: ${modelId}. Attempting to load anyway.`);
      }

      // Test model availability
      if (options.testConnection !== false) {
        await this.testModelAvailability(modelId, modelConfig);
      }

      const model = {
        id: `vertex-ai:${modelId}`,
        provider: 'vertex-ai',
        modelId: modelId,
        type: 'vertex-ai',
        config: {
          projectId: this.projectId,
          location: this.location,
          ...options
        },
        metadata: {
          ...modelConfig,
          streaming: this.supportsStreaming(modelConfig),
          enterprise: true,
          mlops: true,
          gcp_native: true,
          loaded: true,
          loadedAt: Date.now()
        }
      };

      this.models.set(modelId, model);
      this.model = model;
      this.loaded = true;

      logger.success(`âœ… Loaded Vertex AI model: ${model.id}`);
      return model;

    } catch (error) {
      logger.error(`Failed to load Vertex AI model ${modelId}:`, error);
      throw error;
    }
  }

  /**
   * Check if model supports streaming
   */
  supportsStreaming(modelConfig) {
    // Gemini models support streaming, PaLM models currently don't
    return modelConfig?.family === 'gemini';
  }

  /**
   * Test model availability on Vertex AI
   */
  async testModelAvailability(modelId, modelConfig) {
    try {
      // In real implementation, would check model availability via Vertex AI API
      if (!modelConfig) {
        logger.warn(`Model ${modelId} may not be available in ${this.location}`);
      }

      logger.debug(`âœ… Model availability test passed for ${modelId}`);
      return true;
    } catch (error) {
      logger.warn(`âš ï¸ Model availability test failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Generate completion using Vertex AI
   */
  async complete(prompt, options = {}) {
    const model = options.model || this.model;
    
    if (!model) {
      throw new Error('No model loaded');
    }

    // Build Vertex AI request
    const request = this.buildVertexRequest(prompt, model, options);
    const endpoint = this.getVertexEndpoint(model);

    try {
      if (options.stream &amp;&amp; model.metadata.streaming) {
        return await this.streamCompletion(request, endpoint, model, options);
      } else {
        return await this.standardCompletion(request, endpoint, model, options);
      }
    } catch (error) {
      logger.error(`Vertex AI request failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Build Vertex AI request
   */
  buildVertexRequest(prompt, model, options) {
    const modelConfig = VERTEX_AI_MODELS[model.modelId];
    const family = modelConfig?.family || 'gemini';

    const baseRequest = {
      maxOutputTokens: options.maxTokens || modelConfig?.maxOutput || 1000,
      temperature: options.temperature || 0.7,
      topP: options.topP || 1.0,
      topK: options.topK || 40
    };

    switch (family) {
      case 'gemini':
        return this.buildGeminiRequest(prompt, baseRequest, options);
      
      case 'palm':
        return this.buildPaLMRequest(prompt, baseRequest, options);
      
      case 'codey':
        return this.buildCodeyRequest(prompt, baseRequest, options);
      
      case 'embedding':
        return this.buildEmbeddingRequest(prompt, options);
      
      default:
        throw new Error(`Unsupported model family: ${family}`);
    }
  }

  /**
   * Build Gemini request
   */
  buildGeminiRequest(prompt, baseRequest, options) {
    const contents = [];

    // Handle multimodal input
    if (options.images &amp;&amp; options.images.length > 0) {
      contents.push({
        role: 'user',
        parts: [
          { text: prompt },
          ...options.images.map(img => ({
            inlineData: {
              mimeType: img.mimeType || 'image/jpeg',
              data: img.data
            }
          }))
        ]
      });
    } else {
      contents.push({
        role: 'user',
        parts: [{ text: prompt }]
      });
    }

    return {
      contents,
      generationConfig: {
        maxOutputTokens: baseRequest.maxOutputTokens,
        temperature: baseRequest.temperature,
        topP: baseRequest.topP,
        topK: baseRequest.topK,
        stopSequences: options.stopSequences
      },
      safetySettings: options.safetySettings || this.getDefaultSafetySettings()
    };
  }

  /**
   * Build PaLM request
   */
  buildPaLMRequest(prompt, baseRequest, options) {
    const isChat = model.modelId.includes('chat');

    if (isChat) {
      return {
        context: options.context || '',
        examples: options.examples || [],
        messages: options.messages || [
          { author: 'user', content: prompt }
        ],
        temperature: baseRequest.temperature,
        candidateCount: 1,
        topK: baseRequest.topK,
        topP: baseRequest.topP
      };
    } else {
      return {
        prompt: {
          text: prompt
        },
        temperature: baseRequest.temperature,
        maxOutputTokens: baseRequest.maxOutputTokens,
        topK: baseRequest.topK,
        topP: baseRequest.topP,
        candidateCount: 1
      };
    }
  }

  /**
   * Build Codey request
   */
  buildCodeyRequest(prompt, baseRequest, options) {
    const isChat = model.modelId.includes('codechat');

    if (isChat) {
      return {
        messages: options.messages || [
          { author: 'user', content: prompt }
        ],
        temperature: baseRequest.temperature,
        maxOutputTokens: baseRequest.maxOutputTokens,
        candidateCount: 1
      };
    } else {
      return {
        prefix: options.prefix || '',
        suffix: options.suffix || '',
        temperature: baseRequest.temperature,
        maxOutputTokens: baseRequest.maxOutputTokens,
        candidateCount: 1
      };
    }
  }

  /**
   * Build embedding request
   */
  buildEmbeddingRequest(prompt, options) {
    return {
      instances: [
        {
          content: prompt,
          task_type: options.taskType || 'RETRIEVAL_DOCUMENT'
        }
      ]
    };
  }

  /**
   * Get default safety settings for Gemini
   */
  getDefaultSafetySettings() {
    return [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE'
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE'
      },
      {
        category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE'
      },
      {
        category: 'HARM_CATEGORY_HARASSMENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE'
      }
    ];
  }

  /**
   * Get Vertex AI endpoint
   */
  getVertexEndpoint(model) {
    const modelConfig = VERTEX_AI_MODELS[model.modelId];
    const family = modelConfig?.family || 'gemini';
    const version = modelConfig?.version || model.modelId;

    const baseUrl = `https://${this.location}-aiplatform.googleapis.com/v1`;
    const projectPath = `projects/${this.projectId}/locations/${this.location}`;

    switch (family) {
      case 'gemini':
        return `${baseUrl}/${projectPath}/publishers/google/models/${version}:generateContent`;
      
      case 'palm':
        if (model.modelId.includes('chat')) {
          return `${baseUrl}/${projectPath}/publishers/google/models/${version}:predict`;
        } else {
          return `${baseUrl}/${projectPath}/publishers/google/models/${version}:predict`;
        }
      
      case 'codey':
        if (model.modelId.includes('codechat')) {
          return `${baseUrl}/${projectPath}/publishers/google/models/${version}:predict`;
        } else {
          return `${baseUrl}/${projectPath}/publishers/google/models/${version}:predict`;
        }
      
      case 'embedding':
        return `${baseUrl}/${projectPath}/publishers/google/models/${version}:predict`;
      
      default:
        return `${baseUrl}/${projectPath}/publishers/google/models/${version}:predict`;
    }
  }

  /**
   * Standard (non-streaming) completion
   */
  async standardCompletion(request, endpoint, model, options) {
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: this.getHeaders(),
      body: JSON.stringify(request)
    });

    if (!response.ok) {
      await this.handleVertexError(response);
    }

    const data = await response.json();
    return this.parseVertexResponse(data, model);
  }

  /**
   * Streaming completion
   */
  async streamCompletion(request, endpoint, model, options) {
    // Add streaming parameter
    const streamEndpoint = endpoint.replace(':generateContent', ':streamGenerateContent');
    
    const response = await fetch(streamEndpoint, {
      method: 'POST',
      headers: this.getHeaders(),
      body: JSON.stringify(request)
    });

    if (!response.ok) {
      await this.handleVertexError(response);
    }

    return this.handleVertexStream(response, model);
  }

  /**
   * Handle Vertex AI errors
   */
  async handleVertexError(response) {
    const errorText = await response.text();
    let errorData;
    
    try {
      errorData = JSON.parse(errorText);
    } catch {
      errorData = { message: errorText };
    }

    const message = errorData.error?.message || errorData.message || `Vertex AI error (${response.status})`;
    throw new Error(message);
  }

  /**
   * Parse Vertex AI response
   */
  parseVertexResponse(data, model) {
    const modelConfig = VERTEX_AI_MODELS[model.modelId];
    const family = modelConfig?.family || 'gemini';

    switch (family) {
      case 'gemini':
        return this.parseGeminiResponse(data, model);
      
      case 'palm':
        return this.parsePaLMResponse(data, model);
      
      case 'codey':
        return this.parseCodeyResponse(data, model);
      
      case 'embedding':
        return this.parseEmbeddingResponse(data, model);
      
      default:
        throw new Error(`Unsupported model family: ${family}`);
    }
  }

  /**
   * Parse Gemini response
   */
  parseGeminiResponse(data, model) {
    const candidate = data.candidates?.[0];
    const content = candidate?.content;
    const text = content?.parts?.[0]?.text || '';
    
    const usage = data.usageMetadata || {};
    const finishReason = candidate?.finishReason || 'STOP';

    return {
      text,
      model: model.id,
      provider: 'vertex-ai',
      usage: {
        promptTokens: usage.promptTokenCount || 0,
        completionTokens: usage.candidatesTokenCount || 0,
        totalTokens: usage.totalTokenCount || 0
      },
      cost: this.calculateCost(usage, model.modelId),
      finishReason: finishReason.toLowerCase(),
      metadata: {
        safetyRatings: candidate?.safetyRatings,
        citationMetadata: candidate?.citationMetadata
      },
      timestamp: Date.now()
    };
  }

  /**
   * Parse PaLM response
   */
  parsePaLMResponse(data, model) {
    const prediction = data.predictions?.[0];
    const text = prediction?.candidates?.[0]?.output || prediction?.content || '';
    
    return {
      text,
      model: model.id,
      provider: 'vertex-ai',
      usage: {
        promptTokens: 0, // PaLM doesn't provide token counts
        completionTokens: 0,
        totalTokens: 0
      },
      cost: 0,
      finishReason: 'stop',
      metadata: {
        safetyAttributes: prediction?.safetyAttributes
      },
      timestamp: Date.now()
    };
  }

  /**
   * Parse Codey response
   */
  parseCodeyResponse(data, model) {
    const prediction = data.predictions?.[0];
    const text = prediction?.candidates?.[0]?.output || prediction?.content || '';
    
    return {
      text,
      model: model.id,
      provider: 'vertex-ai',
      usage: {
        promptTokens: 0, // Codey doesn't provide token counts
        completionTokens: 0,
        totalTokens: 0
      },
      cost: 0,
      finishReason: 'stop',
      metadata: {
        language: prediction?.language,
        safetyAttributes: prediction?.safetyAttributes
      },
      timestamp: Date.now()
    };
  }

  /**
   * Parse embedding response
   */
  parseEmbeddingResponse(data, model) {
    const prediction = data.predictions?.[0];
    const embeddings = prediction?.embeddings?.values || [];
    
    return {
      embedding: embeddings,
      model: model.id,
      provider: 'vertex-ai',
      usage: {
        promptTokens: 0,
        completionTokens: 0,
        totalTokens: 0
      },
      cost: this.calculateCost({ prompt_tokens: 1 }, model.modelId), // Simplified
      metadata: {
        dimensions: embeddings.length
      },
      timestamp: Date.now()
    };
  }

  /**
   * Handle Vertex AI streaming response
   */
  async *handleVertexStream(response, model) {
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = '';

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.trim() === '') continue;

          try {
            const data = JSON.parse(line);
            const candidate = data.candidates?.[0];
            const content = candidate?.content?.parts?.[0]?.text;
            
            if (content) {
              yield {
                text: content,
                model: model.id,
                provider: 'vertex-ai',
                chunk: true,
                timestamp: Date.now()
              };
            }
          } catch (e) {
            // Skip invalid JSON
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * Calculate cost based on Vertex AI pricing
   */
  calculateCost(usage, modelId) {
    const modelConfig = VERTEX_AI_MODELS[modelId];
    if (!modelConfig || !modelConfig.cost) return 0;

    const inputTokens = usage.promptTokenCount || usage.prompt_tokens || 0;
    const outputTokens = usage.candidatesTokenCount || usage.completion_tokens || 0;
    
    const inputCost = (inputTokens / 1000000) * modelConfig.cost.input;
    const outputCost = (outputTokens / 1000000) * modelConfig.cost.output;
    
    return inputCost + outputCost;
  }

  /**
   * List available Vertex AI models
   */
  async listModels() {
    return Object.keys(VERTEX_AI_MODELS).map(id => ({
      id,
      name: VERTEX_AI_MODELS[id].name,
      provider: 'vertex-ai',
      family: VERTEX_AI_MODELS[id].family,
      location: this.location,
      metadata: VERTEX_AI_MODELS[id]
    }));
  }

  /**
   * Get adapter information
   */
  getInfo() {
    return {
      name: 'VertexAIAdapter',
      version: '1.0.0',
      provider: 'vertex-ai',
      projectId: this.projectId,
      location: this.location,
      modelsLoaded: this.models.size,
      features: ['multimodal', 'enterprise', 'mlops', 'streaming', 'vision', 'embeddings'],
      families: ['gemini', 'palm', 'codey', 'embedding'],
      models: Object.keys(VERTEX_AI_MODELS),
      status: this.vertexClient ? 'ready' : 'initializing'
    };
  }

  /**
   * Unload model
   */
  async unload(modelId) {
    if (this.models.has(modelId)) {
      this.models.delete(modelId);
      logger.info(`Vertex AI model ${modelId} unloaded`);
      return true;
    }
    return false;
  }

  /**
   * Clean up resources
   */
  async dispose() {
    this.models.clear();
    this.tokenCache.clear();
    this.vertexClient = null;
    logger.info('Google Vertex AI adapter disposed');
  }
}

export default VertexAIAdapter;</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="APILoader.html">APILoader</a></li><li><a href="AnthropicAdapter.html">AnthropicAdapter</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthManager.html">AuthManager</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="AzureOpenAIAdapter.html">AzureOpenAIAdapter</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BedrockAdapter.html">BedrockAdapter</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="CohereAdapter.html">CohereAdapter</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="DeepSeekAdapter.html">DeepSeekAdapter</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="ErrorHandler.html">ErrorHandler</a></li><li><a href="FireworksAdapter.html">FireworksAdapter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="GroqAdapter.html">GroqAdapter</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MistralAdapter.html">MistralAdapter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelError.html">ModelError</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelRegistry.html">ModelRegistry</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="NovitaAdapter.html">NovitaAdapter</a></li><li><a href="OpenAIAdapter.html">OpenAIAdapter</a></li><li><a href="OpenRouterAdapter.html">OpenRouterAdapter</a></li><li><a href="PerplexityAdapter.html">PerplexityAdapter</a></li><li><a href="Pipeline.html">Pipeline</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TogetherAdapter.html">TogetherAdapter</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="VertexAIAdapter.html">VertexAIAdapter</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#ADAPTER_REGISTRY">ADAPTER_REGISTRY</a></li><li><a href="global.html#API_KEY_PATTERNS">API_KEY_PATTERNS</a></li><li><a href="global.html#AUTH_TYPES">AUTH_TYPES</a></li><li><a href="global.html#AZURE_API_VERSIONS">AZURE_API_VERSIONS</a></li><li><a href="global.html#AZURE_OPENAI_MODELS">AZURE_OPENAI_MODELS</a></li><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BEDROCK_MODELS">BEDROCK_MODELS</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#CLAUDE_MODELS">CLAUDE_MODELS</a></li><li><a href="global.html#COHERE_MODELS">COHERE_MODELS</a></li><li><a href="global.html#COMPLIANCE_FEATURES">COMPLIANCE_FEATURES</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#DEEPSEEK_ENDPOINTS">DEEPSEEK_ENDPOINTS</a></li><li><a href="global.html#DEEPSEEK_MODELS">DEEPSEEK_MODELS</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#FIREWORKS_MODELS">FIREWORKS_MODELS</a></li><li><a href="global.html#GROQ_MODELS">GROQ_MODELS</a></li><li><a href="global.html#INPUT_TYPES">INPUT_TYPES</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#MISTRAL_MODELS">MISTRAL_MODELS</a></li><li><a href="global.html#MODEL_CATEGORIES">MODEL_CATEGORIES</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#NOVITA_ENDPOINTS">NOVITA_ENDPOINTS</a></li><li><a href="global.html#NOVITA_MODELS">NOVITA_MODELS</a></li><li><a href="global.html#OPENAI_MODELS">OPENAI_MODELS</a></li><li><a href="global.html#PERPLEXITY_MODELS">PERPLEXITY_MODELS</a></li><li><a href="global.html#POPULAR_MODELS">POPULAR_MODELS</a></li><li><a href="global.html#PROVIDER_AUTH_CONFIG">PROVIDER_AUTH_CONFIG</a></li><li><a href="global.html#PROVIDER_CATEGORIES">PROVIDER_CATEGORIES</a></li><li><a href="global.html#PROVIDER_CONFIGS">PROVIDER_CONFIGS</a></li><li><a href="global.html#PROVIDER_FEATURES">PROVIDER_FEATURES</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SAFETY_LEVELS">SAFETY_LEVELS</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TOGETHER_MODELS">TOGETHER_MODELS</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#VERTEX_AI_MODELS">VERTEX_AI_MODELS</a></li><li><a href="global.html#VERTEX_REGIONS">VERTEX_REGIONS</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#adjustTimeouts">adjustTimeouts</a></li><li><a href="global.html#attemptRecovery">attemptRecovery</a></li><li><a href="global.html#checkConnectivity">checkConnectivity</a></li><li><a href="global.html#clearCache">clearCache</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createAdapter">createAdapter</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#createMissingResources">createMissingResources</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#emergencyShutdown">emergencyShutdown</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#escalateError">escalateError</a></li><li><a href="global.html#executeRecovery">executeRecovery</a></li><li><a href="global.html#getAdapter">getAdapter</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#getProviderAuthType">getProviderAuthType</a></li><li><a href="global.html#getProviderInfo">getProviderInfo</a></li><li><a href="global.html#getProvidersByCategory">getProvidersByCategory</a></li><li><a href="global.html#getProvidersByFeature">getProvidersByFeature</a></li><li><a href="global.html#getStats">getStats</a></li><li><a href="global.html#getSupportedProviders">getSupportedProviders</a></li><li><a href="global.html#gracefulShutdown">gracefulShutdown</a></li><li><a href="global.html#handleCriticalError">handleCriticalError</a></li><li><a href="global.html#handleMemoryLeak">handleMemoryLeak</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#isProviderSupported">isProviderSupported</a></li><li><a href="global.html#logError">logError</a></li><li><a href="global.html#performHealthCheck">performHealthCheck</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#reinstallDependencies">reinstallDependencies</a></li><li><a href="global.html#reload">reload</a></li><li><a href="global.html#restartProcess">restartProcess</a></li><li><a href="global.html#retryConnection">retryConnection</a></li><li><a href="global.html#selectRecoveryStrategy">selectRecoveryStrategy</a></li><li><a href="global.html#setupHandlers">setupHandlers</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#softRestart">softRestart</a></li><li><a href="global.html#startHealthMonitoring">startHealthMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Wed Aug 20 2025 19:41:21 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
